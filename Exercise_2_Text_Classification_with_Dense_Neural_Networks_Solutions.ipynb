{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Snb9H4VSveb"
   },
   "source": [
    "# Text Classification using Word Embeddings and Dense Neural Network Models\n",
    "\n",
    "## Building a Hate Speech Classifier\n",
    "\n",
    "Understanding the text content and predicting the sentiment of the reviews is a form of supervised machine learning. To be more specific, we will be using classification models for solving this problem. We will be building an automated hate speech text classification system in subsequent sections. The major steps to achieve this are mentioned as follows.\n",
    "\n",
    "+ Prepare train and test datasets (optionally a validation dataset)\n",
    "+ Pre-process and normalize text documents\n",
    "+ Feature Engineering \n",
    "+ Model training\n",
    "+ Model prediction and evaluation\n",
    "\n",
    "These are the major steps for building our system. Optionally the last step would be to deploy the model in your server or on the cloud. The following figure shows a detailed workflow for building a standard text classification system with supervised learning (classification) models.\n",
    "\n",
    "In our scenario, documents indicate the posts \\ comments and classes indicate the nature of whether the post was a hate speech incited post or not, which can either be hate or nothate making it a binary classification problem. We will build models using deep learning in the subsequent sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57E8NKYFmDhD",
    "outputId": "779d7cf0-cb7d-49a7-8ef2-4c3a1511c1a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar  4 21:52:26 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   36C    P0    37W / 300W |    577MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJjFJX8BSd0Y",
    "outputId": "8157cb9f-8116-473b-f64c-27aca918769b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.48)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.1.7)\n",
      "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.1)\n",
      "Requirement already satisfied: textsearch in /usr/local/lib/python3.7/dist-packages (0.0.21)\n",
      "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch) (1.4.1)\n",
      "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch) (0.1.7)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install textsearch\n",
    "!pip install tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWDsJLO2TgXj"
   },
   "source": [
    "## Load Dataset - Hate Speech\n",
    "\n",
    "Social media unfortunately is rampant with hate speech in the form of posts and comments. This is a practical example of perhaps building an automated hate speech detection system using NLP in the form of text classification.\n",
    "\n",
    "In this notebook, we will leverage an open sourced collection of hate speech posts and comments.\n",
    "\n",
    "The dataset is available here: [kaggle](https://www.kaggle.com/usharengaraju/dynamically-generated-hate-speech-dataset) which in turn has been curated from a wider [data source for hate speech](https://hatespeechdata.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8It-PVfDTe3M",
    "outputId": "54817eec-67fc-4e85-85b1-13aefd24ef34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40623 entries, 0 to 40622\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Unnamed: 0      40623 non-null  int64  \n",
      " 1   id              40623 non-null  object \n",
      " 2   text            40623 non-null  object \n",
      " 3   label           40623 non-null  object \n",
      " 4   type            40623 non-null  object \n",
      " 5   model_wrong     26097 non-null  object \n",
      " 6   db.model_preds  26097 non-null  float64\n",
      " 7   status          40623 non-null  object \n",
      " 8   round           40623 non-null  object \n",
      " 9   split           40623 non-null  object \n",
      " 10  annotator       40623 non-null  object \n",
      "dtypes: float64(1), int64(1), object(9)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('HateDataset.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDdGMU_PPH4R"
   },
   "source": [
    "To keep things simple we will focus on predicting the labels from the text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "f-ffIdbdGAWp",
    "outputId": "9893c630-4e77-44ef-a1af-573bb084a4d6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It should be legal for a husband to rape his wife</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dalits are lowlife scum</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dalits are lowlives</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It was a better world when women dare not ques...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't work this hard so that those immigrant...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  It should be legal for a husband to rape his wife  hate\n",
       "1                            dalits are lowlife scum  hate\n",
       "2                                dalits are lowlives  hate\n",
       "3  It was a better world when women dare not ques...  hate\n",
       "4  I don't work this hard so that those immigrant...  hate"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['text', 'label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwLFUAXBs3s5"
   },
   "source": [
    "### Split data into train-test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "EyEazzL2GiJV"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "OTfYRTIIGjTY"
   },
   "outputs": [],
   "source": [
    "train_reviews, test_reviews, train_labels, test_labels = train_test_split(df.text.values,\n",
    "                                                                          df.label.values,\n",
    "                                                                          test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0WW0v2BYtC9Q",
    "outputId": "39ecb245-50b2-4abc-ca11-9a9e57cff4c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32498, 8125)"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_reviews), len(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99bOmv0rTs5p"
   },
   "source": [
    "## Text Wrangling and Normalization\n",
    "\n",
    "In this section, we will also normalize our corpus by removing accented characters, newline characters and so on. Lets get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nB7gcz5UdffC"
   },
   "source": [
    "### **Question 1**: **Complete** the following utility functions\n",
    "\n",
    "__Hint:__ Use the knowledge gained from NLP-1 or the classification tutorial to solve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "FmhycTAcTf3w"
   },
   "outputs": [],
   "source": [
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    # hint use beautifulsoup to remove html tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    # hint use the normalize function from unicodedata\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def pre_process_corpus(docs):\n",
    "    norm_docs = []\n",
    "    for doc in tqdm(docs):\n",
    "        # strip HTML tags\n",
    "        doc = strip_html_tags(doc)\n",
    "        # remove extra newlines\n",
    "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "        # lower case\n",
    "        doc = doc.lower()\n",
    "        # remove accented characters\n",
    "        doc = remove_accented_chars(doc)\n",
    "        # fix contractions\n",
    "        doc = contractions.fix(doc)\n",
    "        # remove special characters\\whitespaces\n",
    "        # use regex to keep only letters, numbers and spaces\n",
    "        doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, flags=re.I|re.A)\n",
    "        # use regex to remove extra spaces\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove trailing and leading spaces\n",
    "        doc = doc.strip()  \n",
    "\n",
    "        norm_docs.append(doc)\n",
    "  \n",
    "    return norm_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ccste3GtTf08",
    "outputId": "1432895d-21a9-4356-8a00-494756c3771f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4225/32498 [00:00<00:04, 6906.54it/s]/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "100%|██████████| 32498/32498 [00:04<00:00, 6981.13it/s]\n",
      "100%|██████████| 8125/8125 [00:01<00:00, 7505.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.75 s, sys: 344 ms, total: 6.1 s\n",
      "Wall time: 5.75 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "norm_train_reviews = pre_process_corpus(train_reviews)\n",
    "norm_test_reviews = pre_process_corpus(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKKxB6MqT3DW"
   },
   "source": [
    "## Label Encode Class Labels\n",
    "\n",
    "Our dataset has labels in the form of positive and negative classes. We transform them into consumable form by performing label encoding. Label encoding assigns a unique numerical value to each class. For example: \n",
    "``negative: 0 and positive:1``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "--bbxUmyT7qG"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Activation, Dense\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYNNxRtGdnxm"
   },
   "source": [
    "### **Question 2**: **Complete** the following transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tMLYFYekTfyF",
    "outputId": "d5ba951d-c476-4a7f-8f1d-4e31b86c4929"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32498/32498 [00:04<00:00, 7179.25it/s]\n",
      "100%|██████████| 8125/8125 [00:01<00:00, 7415.09it/s]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "# tokenize train reviews & encode train labels\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                       for text in tqdm(norm_train_reviews)]\n",
    "y_train = le.fit_transform(train_labels)\n",
    "# tokenize test reviews & encode test labels\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                       for text in tqdm(norm_test_reviews)]\n",
    "y_test = le.transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvMPHqUhUCxE"
   },
   "source": [
    "## Feature Engineering based on Word2Vec Embeddings\n",
    "\n",
    "In the previous notebook we discussed different word embedding techniques like word2vec, glove, fastText, etc. In this section we will leverage ``gensim`` to transform our dataset into word2vec  representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "IWaE8tMjTx9M"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpQPa5Bid3jB"
   },
   "source": [
    "### **Question 3**: **Get** feature vectors using Word2Vec\n",
    "\n",
    "Build the word2vec model on your tokenized train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4wRI5lPhTx7B",
    "outputId": "62587467-6726-41ad-b821-a46a9ee0c8ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-04 22:39:51,929 : INFO : collecting all words and their counts\n",
      "2021-03-04 22:39:51,931 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-03-04 22:39:51,968 : INFO : PROGRESS: at sentence #10000, processed 191896 words, keeping 12443 word types\n",
      "2021-03-04 22:39:52,000 : INFO : PROGRESS: at sentence #20000, processed 381698 words, keeping 16843 word types\n",
      "2021-03-04 22:39:52,037 : INFO : PROGRESS: at sentence #30000, processed 569659 words, keeping 19666 word types\n",
      "2021-03-04 22:39:52,048 : INFO : collected 20221 word types from a corpus of 616735 raw words and 32498 sentences\n",
      "2021-03-04 22:39:52,048 : INFO : Loading a fresh vocabulary\n",
      "2021-03-04 22:39:52,080 : INFO : effective_min_count=2 retains 13578 unique words (67% of original 20221, drops 6643)\n",
      "2021-03-04 22:39:52,081 : INFO : effective_min_count=2 leaves 610092 word corpus (98% of original 616735, drops 6643)\n",
      "2021-03-04 22:39:52,127 : INFO : deleting the raw counts dictionary of 20221 items\n",
      "2021-03-04 22:39:52,128 : INFO : sample=0.001 downsamples 57 most-common words\n",
      "2021-03-04 22:39:52,129 : INFO : downsampling leaves estimated 444978 word corpus (72.9% of prior 610092)\n",
      "2021-03-04 22:39:52,171 : INFO : estimated required memory for 13578 words and 300 dimensions: 39376200 bytes\n",
      "2021-03-04 22:39:52,172 : INFO : resetting layer weights\n",
      "2021-03-04 22:39:54,929 : INFO : training model with 4 workers on 13578 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=150\n",
      "2021-03-04 22:39:56,033 : INFO : EPOCH 1 - PROGRESS: at 49.98% examples, 208403 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 22:39:56,872 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-04 22:39:56,902 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-04 22:39:56,910 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-04 22:39:56,913 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-04 22:39:56,914 : INFO : EPOCH - 1 : training on 616735 raw words (444454 effective words) took 2.0s, 227749 effective words/s\n",
      "2021-03-04 22:39:57,927 : INFO : EPOCH 2 - PROGRESS: at 48.34% examples, 215325 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 22:39:58,804 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-04 22:39:58,859 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-04 22:39:58,871 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-04 22:39:58,883 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-04 22:39:58,884 : INFO : EPOCH - 2 : training on 616735 raw words (444795 effective words) took 2.0s, 226830 effective words/s\n",
      "2021-03-04 22:39:59,928 : INFO : EPOCH 3 - PROGRESS: at 48.34% examples, 209250 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 22:40:00,778 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-04 22:40:00,818 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-04 22:40:00,840 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-04 22:40:00,842 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-04 22:40:00,843 : INFO : EPOCH - 3 : training on 616735 raw words (445256 effective words) took 1.9s, 228544 effective words/s\n",
      "2021-03-04 22:40:01,895 : INFO : EPOCH 4 - PROGRESS: at 48.34% examples, 207612 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 22:40:02,770 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-04 22:40:02,788 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-04 22:40:02,791 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-04 22:40:02,821 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-04 22:40:02,822 : INFO : EPOCH - 4 : training on 616735 raw words (444862 effective words) took 2.0s, 225813 effective words/s\n",
      "2021-03-04 22:40:03,845 : INFO : EPOCH 5 - PROGRESS: at 48.34% examples, 213329 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 22:40:04,722 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-04 22:40:04,754 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-04 22:40:04,774 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-04 22:40:04,781 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-04 22:40:04,781 : INFO : EPOCH - 5 : training on 616735 raw words (445320 effective words) took 2.0s, 228297 effective words/s\n",
      "2021-03-04 22:40:05,804 : INFO : EPOCH 6 - PROGRESS: at 49.90% examples, 220509 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 22:40:06,709 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-04 22:40:06,723 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-04 22:40:06,741 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-04 22:40:06,748 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-04 22:40:06,749 : INFO : EPOCH - 6 : training on 616735 raw words (444775 effective words) took 2.0s, 227146 effective words/s\n",
      "2021-03-04 22:40:07,766 : INFO : EPOCH 7 - PROGRESS: at 48.46% examples, 214840 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 22:40:08,650 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-04 22:40:08,658 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-04 22:40:08,699 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-04 22:40:08,701 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-04 22:40:08,703 : INFO : EPOCH - 7 : training on 616735 raw words (444887 effective words) took 1.9s, 228811 effective words/s\n",
      "2021-03-04 22:40:09,733 : INFO : EPOCH 8 - PROGRESS: at 48.34% examples, 212126 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 22:40:10,620 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-04 22:40:10,654 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-04 22:40:10,670 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-04 22:40:10,675 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-04 22:40:10,676 : INFO : EPOCH - 8 : training on 616735 raw words (445112 effective words) took 2.0s, 226779 effective words/s\n",
      "2021-03-04 22:40:11,719 : INFO : EPOCH 9 - PROGRESS: at 49.90% examples, 216527 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 22:40:12,559 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-04 22:40:12,605 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-04 22:40:12,616 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-04 22:40:12,629 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-04 22:40:12,630 : INFO : EPOCH - 9 : training on 616735 raw words (444884 effective words) took 1.9s, 229103 effective words/s\n",
      "2021-03-04 22:40:13,668 : INFO : EPOCH 10 - PROGRESS: at 48.34% examples, 209848 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 22:40:14,549 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-04 22:40:14,573 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-04 22:40:14,594 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-04 22:40:14,603 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-04 22:40:14,604 : INFO : EPOCH - 10 : training on 616735 raw words (444422 effective words) took 2.0s, 226372 effective words/s\n",
      "2021-03-04 22:40:14,605 : INFO : training on a 6167350 raw words (4448767 effective words) took 19.7s, 226112 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.9 s, sys: 88.3 ms, total: 41 s\n",
      "Wall time: 22.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build word2vec model\n",
    "w2v_num_features = 300\n",
    "# use a similar config as the tutorial but use a min_count of 2 and train for 10 iterations\n",
    "w2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features, window=150,\n",
    "                                   min_count=2, workers=4, iter=10)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dfG3GqOUMZt"
   },
   "source": [
    "## Averaged Document Vectors\n",
    "\n",
    "A sentence in very simple terms is a collection of words. By now we know how to transform words into vector representation. But how do we transform sentences and documents into vector representation?\n",
    "\n",
    "A simple and naïve way is to average all words in a given sentence to form a sentence vector. In this section, we will leverage this technique itself to prepare our sentence/document vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llYpaGCUs6Kp"
   },
   "source": [
    "### **Question 4**: **Complete** the following utility to build a function to generate and obtain averaged document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "R_hYGOxJTx4S"
   },
   "outputs": [],
   "source": [
    "def averaged_doc_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    \n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "        \n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                nwords = nwords + 1.\n",
    "                feature_vector = np.add(feature_vector, model.wv[word])\n",
    "        if nwords:\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "        return feature_vector\n",
    "\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "NFb8NigUTx1l"
   },
   "outputs": [],
   "source": [
    "# generate averaged word vector features from word2vec model\n",
    "avg_w2v_train_features = averaged_doc_vectorizer(corpus=tokenized_train, model=w2v_model,\n",
    "                                                     num_features=w2v_num_features)\n",
    "avg_w2v_test_features = averaged_doc_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
    "                                                    num_features=w2v_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GJ0ivNX6URz6",
    "outputId": "51365b56-75a3-4b51-decf-8f6d4584fc44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model:> Train features shape: (32498, 300)  Test features shape: (8125, 300)\n"
     ]
    }
   ],
   "source": [
    "print('Word2Vec model:> Train features shape:', avg_w2v_train_features.shape, \n",
    "      ' Test features shape:', avg_w2v_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FepcmBm4Udqq"
   },
   "source": [
    "## Define DNN Model\n",
    "\n",
    "Let us leverage ``tensorflow.keras`` to build our deep neural network for movie review classification task.\n",
    "We will make use of ``Dense`` layers with ``ReLU`` activation and ``Dropout`` to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOWow1aAeLBf"
   },
   "source": [
    "### **Question 5**: **Complete** the following utility to build a deep neural network for classification task\n",
    "\n",
    "Use a similar architecture as the tutorial, key components listed below for reference:\n",
    "\n",
    "- 3 Dense Layers\n",
    "- 512 - 256 - 256 (neurons)\n",
    "- 20% dropout in each layer\n",
    "- 1 output layer for binary classification\n",
    "- binary crossentropy loss \n",
    "- adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "ahPQCuQRURxI"
   },
   "outputs": [],
   "source": [
    "def construct_deepnn_architecture(num_input_features):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(512, input_shape=(num_input_features,)))\n",
    "    dnn_model.add(Activation('relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    \n",
    "    dnn_model.add(Dense(256))\n",
    "    dnn_model.add(Activation('relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    \n",
    "    dnn_model.add(Dense(256))\n",
    "    dnn_model.add(Activation('relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    \n",
    "    dnn_model.add(Dense(1))\n",
    "    dnn_model.add(Activation('sigmoid'))\n",
    "\n",
    "    dnn_model.compile(loss='binary_crossentropy', optimizer='adam',                 \n",
    "                      metrics=['accuracy'])\n",
    "    return dnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rn4ISD4UjZO"
   },
   "source": [
    "## Compile and Visualize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "g39TIs2-URuz"
   },
   "outputs": [],
   "source": [
    "w2v_dnn = construct_deepnn_architecture(num_input_features=w2v_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uI9ScMZwURr9",
    "outputId": "42388bb5-e43e-4dc0-d22b-9d7de9daaca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 512)               154112    \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 351,489\n",
      "Trainable params: 351,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "w2v_dnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdDbvOuoUqdM"
   },
   "source": [
    "## Train the Model using Word2Vec Features\n",
    "\n",
    "The first exercise is to leverage word2vec features as input to our deep neural network to perform moview review classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8rvfZLseVsI"
   },
   "source": [
    "### **Question 6**: **Train** the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wfTB6mqVURpB",
    "outputId": "d739bf24-336b-4bc0-db7c-0447f00043a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "457/457 [==============================] - 2s 3ms/step - loss: 0.6595 - accuracy: 0.5877 - val_loss: 0.6324 - val_accuracy: 0.6178\n",
      "Epoch 2/15\n",
      "457/457 [==============================] - 1s 3ms/step - loss: 0.6231 - accuracy: 0.6332 - val_loss: 0.6176 - val_accuracy: 0.6434\n",
      "Epoch 3/15\n",
      "457/457 [==============================] - 1s 3ms/step - loss: 0.6080 - accuracy: 0.6466 - val_loss: 0.6034 - val_accuracy: 0.6529\n",
      "Epoch 4/15\n",
      "457/457 [==============================] - 1s 3ms/step - loss: 0.5919 - accuracy: 0.6607 - val_loss: 0.5893 - val_accuracy: 0.6585\n",
      "Epoch 5/15\n",
      "457/457 [==============================] - 1s 3ms/step - loss: 0.5813 - accuracy: 0.6734 - val_loss: 0.5884 - val_accuracy: 0.6622\n",
      "Epoch 6/15\n",
      "457/457 [==============================] - 1s 3ms/step - loss: 0.5716 - accuracy: 0.6763 - val_loss: 0.5781 - val_accuracy: 0.6757\n",
      "Epoch 7/15\n",
      "457/457 [==============================] - 1s 3ms/step - loss: 0.5545 - accuracy: 0.6862 - val_loss: 0.5690 - val_accuracy: 0.6757\n",
      "Epoch 8/15\n",
      "457/457 [==============================] - 1s 3ms/step - loss: 0.5510 - accuracy: 0.6920 - val_loss: 0.5651 - val_accuracy: 0.6809\n",
      "Epoch 9/15\n",
      "457/457 [==============================] - 1s 3ms/step - loss: 0.5332 - accuracy: 0.7059 - val_loss: 0.5690 - val_accuracy: 0.6766\n",
      "Epoch 10/15\n",
      "457/457 [==============================] - 1s 3ms/step - loss: 0.5322 - accuracy: 0.7045 - val_loss: 0.5576 - val_accuracy: 0.6948\n",
      "Epoch 11/15\n",
      "457/457 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7180 - val_loss: 0.5679 - val_accuracy: 0.6843\n",
      "Epoch 12/15\n",
      "457/457 [==============================] - 1s 3ms/step - loss: 0.5136 - accuracy: 0.7177 - val_loss: 0.5596 - val_accuracy: 0.6929\n",
      "Epoch 13/15\n",
      "457/457 [==============================] - 1s 3ms/step - loss: 0.5078 - accuracy: 0.7216 - val_loss: 0.5662 - val_accuracy: 0.6908\n",
      "Epoch 14/15\n",
      "457/457 [==============================] - 1s 3ms/step - loss: 0.5014 - accuracy: 0.7236 - val_loss: 0.5684 - val_accuracy: 0.6818\n",
      "Epoch 15/15\n",
      "457/457 [==============================] - 1s 3ms/step - loss: 0.5000 - accuracy: 0.7267 - val_loss: 0.5766 - val_accuracy: 0.6908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6b3fa9eb90>"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "w2v_dnn.fit(avg_w2v_train_features, y_train, epochs=15, batch_size=batch_size, \n",
    "            shuffle=True, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KN-btNFUuxX"
   },
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "ZSAIE0_3nMYE"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zrs3GX0lUqDL",
    "outputId": "7ca557fa-95ec-4549-e2da-b83c299ed93a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:289: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "y_pred = w2v_dnn.predict_classes(avg_w2v_test_features)\n",
    "predictions = le.inverse_transform(y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suXs6CztelVX"
   },
   "source": [
    "### **Question 7**: **Get** evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "id": "jKixXpo0UqAw",
    "outputId": "809fe736-f3eb-4c3a-af7b-1f7c7441e83f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        hate       0.67      0.83      0.74      4401\n",
      "     nothate       0.72      0.52      0.60      3724\n",
      "\n",
      "    accuracy                           0.69      8125\n",
      "   macro avg       0.70      0.67      0.67      8125\n",
      "weighted avg       0.69      0.69      0.68      8125\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hate</th>\n",
       "      <th>nothate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hate</th>\n",
       "      <td>3655</td>\n",
       "      <td>746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nothate</th>\n",
       "      <td>1800</td>\n",
       "      <td>1924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         hate  nothate\n",
       "hate     3655      746\n",
       "nothate  1800     1924"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = le.classes_.tolist()\n",
    "# print classification report\n",
    "print(classification_report(test_labels, predictions))\n",
    "# display confusion matrix\n",
    "pd.DataFrame(confusion_matrix(test_labels, predictions), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkC5J8vos_gc"
   },
   "source": [
    "Congratulations you have built your first hate speech detection model!\n",
    "\n",
    "We will look at more complex models in the future to see if we can improve this performance given this is a pretty complex dataset \\ domain as compared to basic sentiment analysis"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise 2 - Text Classification with Dense Neural Networks Solutions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
