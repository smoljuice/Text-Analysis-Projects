{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sjp7WlCGajaa"
   },
   "source": [
    "# Exercise 1 - Text Classification using Pretrained Embeddings\n",
    "\n",
    "Handling tough real-world problems in Natural Language Processing (NLP) include tackling with class imbalance and the lack of availability of enough labeled data for training. Thanks to the recent advancements in deep transfer learning in NLP, we have been able to make rapid strides in not only tackling these problems but also leverage these models for diverse downstream NLP tasks.\n",
    "\n",
    "The intent of this notebook is to look at various SOTA models in deep transfer learning for NLP with hands-on examples:\n",
    "\n",
    "- Pre-trained word embeddings for Deep Learning Models (FastText with CNNs)\n",
    "- Universal Embeddings (Sentence Encoders, NNLMs)\n",
    "\n",
    "We will take a benchmark classification dataset and train and compare the performance of these models. All examples here will be showcased using Python and leveraging the latest and best of TensorFlow 2.x.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEQr8nEx-sRR"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OQZquMCajaf"
   },
   "source": [
    "# Load Necessary Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8WniVpUajaf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXp8U9ENajah"
   },
   "outputs": [],
   "source": [
    "print(\"TF Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"TF Hub version: \", hub.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IYUA5Zlajak"
   },
   "source": [
    "## Load Dataset - Hate Speech\n",
    "\n",
    "Social media unfortunately is rampant with hate speech in the form of posts and comments. This is a practical example of perhaps building an automated hate speech detection system using NLP in the form of text classification.\n",
    "\n",
    "In this notebook, we will leverage an open sourced collection of hate speech posts and comments.\n",
    "\n",
    "The dataset is available here: [kaggle](https://www.kaggle.com/usharengaraju/dynamically-generated-hate-speech-dataset) which in turn has been curated from a wider [data source for hate speech](https://hatespeechdata.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V20BvLjjajal"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('HateDataset.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDDJCvjrajan"
   },
   "outputs": [],
   "source": [
    "df = df[['text', 'label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdMiB1EFajap"
   },
   "source": [
    "# Preparing Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BiHTDHD_ajap"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_reviews, test_reviews, train_labels, test_labels = train_test_split(df.text.values,\n",
    "                                                                          df.label.values,\n",
    "                                                                          test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28dKBiUrO9ua"
   },
   "outputs": [],
   "source": [
    "len(train_reviews), len(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXDBz_qqajas"
   },
   "source": [
    "# Basic Text Pre-processing\n",
    "\n",
    "We do minimal text pre-processing here since we are using deep learning models and not count-based methods. Steps include the following:\n",
    "\n",
    "- Removing HTML characters\n",
    "- Converting accented characters\n",
    "- Fixing contractions\n",
    "- Removing special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-xMIu2Najas"
   },
   "outputs": [],
   "source": [
    "!sudo pip3 install contractions\n",
    "!sudo pip3 install textsearch\n",
    "!sudo pip3 install tqdm\n",
    "!sudo pip3 install nltk\n",
    "!sudo pip3 install beautifulsoup4\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcAsV316Ar8X"
   },
   "source": [
    "# **Question 1**: Build the text pre-processing pipeline (3 points)\n",
    "\n",
    "__Hint:__ You can follow the same sequence of steps like the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0le4IFlxajau"
   },
   "outputs": [],
   "source": [
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    <YOUR CODE HERE>\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    <YOUR CODE HERE>\n",
    "\n",
    "def pre_process_corpus(docs):\n",
    "    norm_docs = []\n",
    "    <YOUR CODE HERE>\n",
    "    return norm_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCa_kwSoajaw"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "norm_train_reviews = <YOUR CODE HERE>\n",
    "norm_test_reviews = <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0Dpkx4jPlKO"
   },
   "source": [
    "## Label Encode Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxE_7E5fBCWg"
   },
   "source": [
    "# **Question 2**: Label Encode Class Labels (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PaO4N2K9PoFv"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# positive -> 1, negative -> 0\n",
    "\n",
    "le = <YOUR CODE HERE>\n",
    "num_classes = <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLKdlUs-PxRD"
   },
   "outputs": [],
   "source": [
    "y_train = <YOUR CODE HERE>\n",
    "y_test = <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAJy5oNXcWzx"
   },
   "source": [
    "# __Question 3:__ Build Model 0 - Simple Baseline ML Model - Logistic Regression (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orFz4oyrhRWz"
   },
   "source": [
    "## Feature Extraction with BOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CKxZvOccDl7"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = <YOUR CODE HERE>\n",
    "\n",
    "cv_train_features = <YOUR CODE HERE>\n",
    "cv_test_features = <YOUR CODE HERE>\n",
    "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTJzeCmAhVqf"
   },
   "source": [
    "## Train the ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGK-qpVFcgAk"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Logistic Regression model on BOW features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate model\n",
    "lr = <YOUR CODE HERE>\n",
    "\n",
    "# train model\n",
    "<YOUR CODE HERE>\n",
    "\n",
    "# predict on test data\n",
    "lr_bow_predictions = <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruF1eMNLhXp5"
   },
   "source": [
    "## Predict and Test Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKcDYnNsco15"
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beXKvI1QARmW"
   },
   "source": [
    "___Not that great a performance! Can we do better?___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhKn8guEajay"
   },
   "source": [
    "# __Question 4:__ Build Model 1: FastText Embeddings + CNN (4 points)\n",
    "\n",
    "![](https://i.imgur.com/6Pk3Nrv.png)\n",
    "\n",
    "Convolutional Neural Networks (CNNs) have also proven to be very effective in text classification besides computer vision tasks. The idea is to leverage embeddings as features for text data and apply convolutions and poolings on them.\n",
    "\n",
    "We will leverage the ``tensorflow.keras`` utilities to tokenize text before we use the FastText embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMCYxEQqajaz"
   },
   "source": [
    "## Tokenizing text to create vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lwnUNVSBQSy"
   },
   "source": [
    "### Tokenize text corpus.\n",
    "_Hint: Use ``tf.keras.preprocessing.text.Tokenizer``_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cj0KZG_2ajaz"
   },
   "outputs": [],
   "source": [
    "t = <YOUR CODE HERE>\n",
    "# fit the tokenizer on the documents\n",
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2hvcxYvaja4"
   },
   "source": [
    "## Convert texts (sequences of words) to sequence of numeric ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfCTE0AKaja4"
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Rhom3wvaja6"
   },
   "outputs": [],
   "source": [
    "print(\"Vocabulary size={}\".format(len(t.word_index)))\n",
    "print(\"Number of Documents={}\".format(t.document_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeEoOXqdaja8"
   },
   "source": [
    "## Visualizing sentence length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QByCrNcDaja8"
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWrzX3eYaja-"
   },
   "source": [
    "## Padding text sequences\n",
    "\n",
    "We standardize the sentence lengths by defining a maximum length. Sentences longer than this are truncated while shorter ones are padded.\n",
    "\n",
    "___Use a max sequence length of around 250 based on the above histogram___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_ALlOn6aja_"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = <YOUR CODE HERE>\n",
    "\n",
    "# pad dataset to a maximum review length in words\n",
    "<YOUR CODE HERE>\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rctc4SDbajbB"
   },
   "source": [
    "## Building FastText based Embedding Matrix\n",
    "\n",
    "Here we will build an embedding matrix based on pre-trained FastText Embeddings available __[here](https://fasttext.cc/docs/en/english-vectors.html)__.\n",
    "\n",
    "We will be using the __wiki-news-300d-1M.vec.zip__ embedding file which has 1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).\n",
    "\n",
    "![](https://i.imgur.com/5de9N5R.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NUo94CZhhLW"
   },
   "source": [
    "## Download Pre-trained FastText Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZP8QYOCA8kg"
   },
   "source": [
    "We have chosen a slightly less powerful model which should download faster than the tutorial but feel free to play around with different pretrained embeddings from [here](https://fasttext.cc/docs/en/english-vectors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUVQNz32dOnD"
   },
   "outputs": [],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKLwq0nFdWEb"
   },
   "outputs": [],
   "source": [
    "!unzip wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSeBiAiqhlR_"
   },
   "source": [
    "## Generate Pre-trained Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FatWEAzF34n"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(t.word_index)\n",
    "EMBED_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mz5A60JEajbD"
   },
   "outputs": [],
   "source": [
    "word2idx = t.word_index\n",
    "FASTTEXT_INIT_EMBEDDINGS_FILE = './wiki-news-300d-1M.vec'\n",
    "\n",
    "\n",
    "def load_pretrained_embeddings(word_to_index, max_features, embedding_size, embedding_file_path):  \n",
    "    \"\"\"\n",
    "    Utility function to load the pre-trained embeddings\n",
    "    \"\"\"  \n",
    "    \n",
    "    <YOUR CODE HERE>\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WfOQV3lajbF"
   },
   "outputs": [],
   "source": [
    "# get FastText embeddings based on our word to index mapping dictionary\n",
    "ft_embeddings = <YOUR CODE HERE>\n",
    "ft_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEV8ZEN8ajbH"
   },
   "source": [
    "## Build Model Architecture\n",
    "\n",
    "We will use the ``tensorflow.keras`` high level API for building our deep neural network. One slight modification is required for the ``Embedding`` layer. In place of initializing this layer with random weights (as is usual), we start from FastText embeddings weights by setting the ``weights`` parameter. We also keep ``trainable`` parameter as ``True`` in order to learn/improve the pretrained weights as per our corpus. The rest of the model has usual ``Conv1D`` and ``MaxPool`` layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdwiN5AvBh-_"
   },
   "source": [
    "### Build a 1D-Convolution based classification model. Initialize the embedding layer with FastText weights\n",
    "\n",
    "___You can use a similar architecture as the tutorial or build your own!___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gbpb7gd3ajbH"
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgU-nHZ2ajbJ"
   },
   "source": [
    "## Train and Validate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSfxfmWKB2yR"
   },
   "source": [
    "### Train the Model\n",
    "\n",
    "Use a similar methodology as the tutorial but use the following configs also:\n",
    "- __`validation_split`__ of __0.02__ i.e. 2%\n",
    "- 5 epochs\n",
    "- 128 batch size\n",
    "- no callbacks needed to keep things simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IpegUjivajbJ"
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-8mIjSHajbL"
   },
   "source": [
    "## Model Performance Evaluation on the Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Omex_E4B7zC"
   },
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXJm1NEHajbL"
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKFT6X21o1Wf"
   },
   "source": [
    "___Do you observe a better performance?___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHgcPoPoajb-"
   },
   "source": [
    "# __Question 5:__ Build Model 2: Neural Network Language Model (4 points)\n",
    "\n",
    "Authors Bengio et. al. in their paper titled [A Neural Probabilistic Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) present a novel method to learn the joint probability function of sequences of\n",
    "words in a language. This LM learns useful representation of sentences and words which can be leveraged for other NLP tasks such as Classication, Translation, etc.\n",
    "\n",
    "Let us leverage NNLM embeddings to train a classifier on the hate speech dataset\n",
    "\n",
    "![](https://i.imgur.com/blaLxUp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpIVtIJ6ajb_"
   },
   "source": [
    "## Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcXeFfnlFDNf"
   },
   "outputs": [],
   "source": [
    "norm_train_reviews = np.array(norm_train_reviews)\n",
    "norm_test_reviews = np.array(norm_test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f94Uo3GNajcC"
   },
   "source": [
    "## Build a NNLM Embedding Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvwlBRF9ajcD"
   },
   "outputs": [],
   "source": [
    "model = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\"\n",
    "hub_layer = <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tH3T6A_PajcF"
   },
   "source": [
    "## Build Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idbNviTuCCtX"
   },
   "source": [
    "### Build a Classification Model using the TF_Hub pretrained model\n",
    "\n",
    "___Use a similar architecture as the tutorial or try your own!___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5QlogyoajcG"
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5MprbzDajcH"
   },
   "source": [
    "## Train and Validate Model\n",
    "\n",
    "Use a similar methodology as the tutorial but use the following configs also:\n",
    "- __`validation_split`__ of __0.02__ i.e. 2%\n",
    "- 5 epochs\n",
    "- 128 batch size\n",
    "- no callbacks needed to keep things simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PnrvpPlajcJ"
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N68uqUGGajcK"
   },
   "source": [
    "## Model Performance Evaluation on the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x008skIhajcK"
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBP41pQ2ajbo"
   },
   "source": [
    "# __Question 6:__ Build Model 3: Google's Universal Sentence Encoder (4 points)\n",
    "\n",
    "These models take as input English strings and\n",
    "produce as output a fixed dimensional embedding\n",
    "representation of the string.\n",
    "\n",
    "It has two models for encoding\n",
    "sentences into embedding vectors. \n",
    "- One makes use\n",
    "of the transformer (Vaswani et al., 2017) architecture\n",
    "- The other is formulated as a deep averaging network (DAN) (Iyyer et al., 2015)\n",
    "\n",
    "__Methodology 1: Transformers__\n",
    "\n",
    "- The transformer based sentence encoding model\n",
    "constructs sentence embeddings using the encoding sub-graph of the transformer architecture\n",
    "(Vaswani et al., 2017). \n",
    "- This sub-graph uses attention to compute context aware representations\n",
    "of words in a sentence that take into account both\n",
    "the ordering and identity of all the other words.\n",
    "- The context aware word representations are converted to a fixed length sentence encoding vector\n",
    "by computing the element-wise sum of the representations at each word position\n",
    "- The encoder takes as input a lowercased (Penn TreeBank) PTB tokenized string\n",
    "and outputs a 512 dimensional vector as the sentence embedding\n",
    "\n",
    "\n",
    "__Methodology 2: Deep Averaging Network (DAN)__\n",
    "\n",
    "- In the deep averaging network (DAN) (Iyyer et al.,\n",
    "2015) the input embeddings for words and\n",
    "bi-grams are first averaged together and then\n",
    "passed through a feedforward deep neural network\n",
    "(DNN) to produce sentence embeddings. \n",
    "- Similar to the Transformer encoder, the DAN encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional sentence embedding.\n",
    "\n",
    "__Training Methodology:__\n",
    "\n",
    "The encoding model is designed to be as general purpose as possible. This is accomplished by using multi-task learning whereby a single encoding model is used to feed multiple downstream tasks. \n",
    "\n",
    "Unsupervised training data for the sentence encoding models are drawn from a variety of web sources. The sources are Wikipedia, web news,\n",
    "web question-answer pages and discussion forums. We augment unsupervised learning with training on supervised data from the Stanford Natural Language Inference (SNLI) corpus.\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/HIeb3tY.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7D93uotFajbx"
   },
   "source": [
    "## Build a USE Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwwGuYNWCU1Y"
   },
   "source": [
    "### Using Tensorflow hub, prepare an instance of ``hub.KerasLayer`` to get sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1OT_A00jTHy"
   },
   "outputs": [],
   "source": [
    "model = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "hub_layer = <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYjvFE7dajb0"
   },
   "source": [
    "## Build Model Architecture\n",
    "\n",
    "___Use a similar architecture as the tutorial or try your own!___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3nWJfhdjp8w"
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjYqKLkMajb2"
   },
   "source": [
    "## Train and Validate Model\n",
    "\n",
    "Use a similar methodology as the tutorial but use the following configs also:\n",
    "- __`validation_split`__ of __0.02__ i.e. 2%\n",
    "- 5 epochs\n",
    "- 128 batch size\n",
    "- no callbacks needed to keep things simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPGdUWzFj5al"
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmPBH7Tfajb8"
   },
   "source": [
    "## Model Performance Evaluation on the Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSWps6DsgqoO"
   },
   "source": [
    "### **Question 7**: Get Evaluation Results of the model (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "febzhocgknWs"
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise 1 - Text Classification with Deep Transfer Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
