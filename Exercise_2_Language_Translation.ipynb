{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kq2zfPShu_-k"
   },
   "source": [
    "# Language Translation with Word Level seq2seq DL Models\n",
    "The objective is to convert a German sentence (sequence of words) to English using a Neural Machine Translation (NMT) system based on word level encoder-decoder models.\n",
    "\n",
    "We will use __Spanish-English__ sentence pairs data from http://www.manythings.org/anki/\n",
    "\n",
    "Sequence-to-Sequence (seq2seq) models are used for a variety of NLP tasks, such as text summarization, speech recognition, language translation, text-to-speech, speech-to-text among others. Our aim is to translate german to english sentences.\n",
    "\n",
    "Here, both, the input and output are sentences. In other words, these sentences are a sequence of words going in and out of our model.\n",
    "\n",
    "<img src=\"https://i.imgur.com/Uk1tCPo.png\">\n",
    "\n",
    "A typical seq2seq model is also known as an encoder-decoder model and has 2 major components:\n",
    "\n",
    "+ The encoder\n",
    "+ The decoder\n",
    "\n",
    "Both these parts are essentially two different sequential models like RNNs\\LSTMs which are combined together.\n",
    "\n",
    "<img src=\"https://i.imgur.com/bT6PAtv.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTOPpFv4vQZ2"
   },
   "source": [
    "## Download the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZjoxDjWXJbn"
   },
   "outputs": [],
   "source": [
    "!wget http://www.manythings.org/anki/spa-eng.zip\n",
    "!unzip spa-eng.zip\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55sPB5nyvUcL"
   },
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAtQsFBFX9g8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "FILE_NAME = './spa.txt'\n",
    "with open(FILE_NAME, mode='rt', encoding='utf-8') as infile:\n",
    "  data = infile.read()\n",
    "  sentences = data.strip().split('\\n')\n",
    "  sentences = [item.split('\\t') for item in sentences]\n",
    "  eng_spa = np.array(sentences)\n",
    "\n",
    "eng_spa = eng_spa[:50000]\n",
    "eng_spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VoEsx43UX9eU"
   },
   "outputs": [],
   "source": [
    "spa = eng_spa[:,1]\n",
    "eng = eng_spa[:,0]\n",
    "eng, spa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKR5RqfQva1R"
   },
   "source": [
    "## Basic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SvzsJw7X9bt"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Remove punctuation and lowercase\n",
    "eng = np.array([s.translate(str.maketrans('', '', string.punctuation)).lower() for s in eng])\n",
    "spa = np.array([s.translate(str.maketrans('', '', string.punctuation)).lower() for s in spa])\n",
    "eng, spa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRoKA94HveAJ"
   },
   "source": [
    "## Analyze Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Q8fHZiLwQjm"
   },
   "source": [
    "### **Question 1**: Get maximum sentence length for both __English__ and __Spanish__ (2 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djWZJln6X9ZK"
   },
   "outputs": [],
   "source": [
    "# Add your code here\n",
    "eng_len = <YOUR CODE HERE>\n",
    "spa_len = <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_us22QO3X9Wc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "length_df = pd.DataFrame({'eng':eng_len, 'spa':spa_len})\n",
    "length_df.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7drStS73viT5"
   },
   "source": [
    "## Transform Text to Sequence of Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3NlxRpOwdyy"
   },
   "source": [
    "### **Question 2**: Use **``tensorflow.keras.tokenizer``** to tokenize and prepare a text to number mapping (2 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWjSGrUiYbL8"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Add your code here\n",
    "eng_tokenizer = <YOUR CODE HERE>\n",
    "eng_tokenizer.<YOUR CODE HERE>\n",
    "eng_vocab_size = <YOUR CODE HERE>\n",
    "eng_length = <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TDQO_iBYbI_"
   },
   "outputs": [],
   "source": [
    "print('Max eng text length:', eng_length)\n",
    "print('English Vocabulary Size:', eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1UOMDq5YbGT"
   },
   "outputs": [],
   "source": [
    "spa_tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "spa_tokenizer.fit_on_texts(spa)\n",
    "spa_vocab_size = len(spa_tokenizer.word_index) + 1\n",
    "spa_length = max(spa_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SF2A1m2YbDm"
   },
   "outputs": [],
   "source": [
    "print('Max deu text length:', spa_length)\n",
    "print('Spanish Vocabulary Size:', spa_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H58tl2CBvmCA"
   },
   "source": [
    "## Prepare Train Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVHHsuALYbA7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_spa, test_spa, train_eng, test_eng = train_test_split(spa, eng, test_size=0.2, random_state = 42)\n",
    "train_spa.shape, test_spa.shape, train_eng.shape, test_eng.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDuZ02kNvpyI"
   },
   "source": [
    "## Normalize Sequence Lengths\n",
    "We normalize sentence lengths by defining the maximum length. Larger sentences/sequences get truncated while smaller ones are padded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swgXAAwnwwVf"
   },
   "source": [
    "### **Question 3**: Normalize sentences by defining the max length. **Pad** shorter ones and truncate the longer ones. (2 points)\n",
    "\n",
    "_Hint: use ``pad_sequences``_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKLJJadVYa-S"
   },
   "outputs": [],
   "source": [
    "# Add your code here\n",
    "<YOUR CODE HERE>\n",
    "\n",
    "train_spa_enc.shape, train_eng_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbhtwLtFY6jF"
   },
   "outputs": [],
   "source": [
    "# Add your code here\n",
    "<YOUR CODE HERE>\n",
    "\n",
    "test_spa_enc.shape, test_eng_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmhm-nWvvtck"
   },
   "source": [
    "## Build the Model\n",
    "\n",
    "<img src=\"https://i.imgur.com/3ZVi97s.png\">\n",
    "\n",
    "+ For the encoder, we will use an embedding layer and an LSTM layer\n",
    "+ For the decoder, we will use another LSTM layer followed by a dense layer\n",
    "+ Repeat Vector helps pass the output sequence from encoder to all LSTM cells in the decoder\n",
    "\n",
    "We leverage the full power of GPUs by using the CUDA variant of the LSTM models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oh2giIxMxCql"
   },
   "source": [
    "### **Question 4**: Define a **Seq2Seq** model with encoder and decoder components (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYOViShiY6go"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 512\n",
    "LSTM_UNITS = 512\n",
    "\n",
    "# Add your code here\n",
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBf3-ECqvw6W"
   },
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pJ5KKmcY6d5"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_spa_enc, train_eng_enc.reshape(train_eng_enc.shape[0], train_eng_enc.shape[1], 1), \n",
    "          epochs=50, batch_size=256, \n",
    "          validation_split = 0.1,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsFTauzMY6be"
   },
   "outputs": [],
   "source": [
    "model.save('my_nmt_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSEmm09Tv2UB"
   },
   "source": [
    "## Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FrnTmsJMY6Y1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train','validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuykkLcKv5IY"
   },
   "source": [
    "## Text Generator\n",
    "We build a utility function to generate text sequences based on the output prediction sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKIE3wknZU1N"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "eng_idx2word = {v:k for k, v in eng_tokenizer.word_index.items()}\n",
    "\n",
    "def generate_prediction_texts(pred_seqs, idx2word_map):\n",
    "  pred_texts = []\n",
    "\n",
    "  for pred in tqdm(preds):\n",
    "    temp = []\n",
    "    for idx in range(len(pred)):\n",
    "      w = eng_idx2word.get(pred[idx], None)\n",
    "      if idx > 0:\n",
    "        if (w == eng_idx2word.get(pred[idx-1], None)) or (w == None):\n",
    "            temp.append('')\n",
    "        else:\n",
    "            temp.append(w)\n",
    "          \n",
    "      else:\n",
    "        if(w == None):\n",
    "            temp.append('')\n",
    "        else:\n",
    "            temp.append(w)            \n",
    "        \n",
    "    pred_texts.append(' '.join(temp))\n",
    "  return pred_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06wIOLokv8Fz"
   },
   "source": [
    "## Translation using seed from Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWJZkPNKZUyi"
   },
   "outputs": [],
   "source": [
    "train_spa_enc[:1000,:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVrfdL91xQFx"
   },
   "source": [
    "### **Question 5**: **Evaluate** model predictions using training samples only (3 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dzmo-VoMZUv6"
   },
   "outputs": [],
   "source": [
    "# Add your code here\n",
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1o2v68KSv_NF"
   },
   "source": [
    "## Translation on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJE_t95wxY5m"
   },
   "source": [
    "### **Question 6**: **Evaluate** model performance on test data (3 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRXKig2hZl1A"
   },
   "outputs": [],
   "source": [
    "# Add your code here\n",
    "<YOUR CODE HERE>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise 2 - Language Translation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
