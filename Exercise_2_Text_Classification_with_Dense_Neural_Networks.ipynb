{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Snb9H4VSveb"
   },
   "source": [
    "# Text Classification using Word Embeddings and Dense Neural Network Models\n",
    "\n",
    "## Building a Hate Speech Classifier\n",
    "\n",
    "Understanding the text content and predicting the sentiment of the reviews is a form of supervised machine learning. To be more specific, we will be using classification models for solving this problem. We will be building an automated hate speech text classification system in subsequent sections. The major steps to achieve this are mentioned as follows.\n",
    "\n",
    "+ Prepare train and test datasets (optionally a validation dataset)\n",
    "+ Pre-process and normalize text documents\n",
    "+ Feature Engineering \n",
    "+ Model training\n",
    "+ Model prediction and evaluation\n",
    "\n",
    "These are the major steps for building our system. Optionally the last step would be to deploy the model in your server or on the cloud. The following figure shows a detailed workflow for building a standard text classification system with supervised learning (classification) models.\n",
    "\n",
    "In our scenario, documents indicate the posts \\ comments and classes indicate the nature of whether the post was a hate speech incited post or not, which can either be hate or nothate making it a binary classification problem. We will build models using deep learning in the subsequent sections.\n",
    "\n",
    "__Fill the sections marked with blanks or `<YOUR CODE HERE>`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57E8NKYFmDhD"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJjFJX8BSd0Y"
   },
   "outputs": [],
   "source": [
    "!pip install contractions\n",
    "!pip install textsearch\n",
    "!pip install tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWDsJLO2TgXj"
   },
   "source": [
    "## Load Dataset - Hate Speech\n",
    "\n",
    "Social media unfortunately is rampant with hate speech in the form of posts and comments. This is a practical example of perhaps building an automated hate speech detection system using NLP in the form of text classification.\n",
    "\n",
    "In this notebook, we will leverage an open sourced collection of hate speech posts and comments.\n",
    "\n",
    "The dataset is available here: [kaggle](https://www.kaggle.com/usharengaraju/dynamically-generated-hate-speech-dataset) which in turn has been curated from a wider [data source for hate speech](https://hatespeechdata.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8It-PVfDTe3M"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('HateDataset.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDdGMU_PPH4R"
   },
   "source": [
    "To keep things simple we will focus on predicting the labels from the text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-ffIdbdGAWp"
   },
   "outputs": [],
   "source": [
    "df = df[['text', 'label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwLFUAXBs3s5"
   },
   "source": [
    "### Split data into train-test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyEazzL2GiJV"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTfYRTIIGjTY"
   },
   "outputs": [],
   "source": [
    "train_reviews, test_reviews, train_labels, test_labels = train_test_split(df.text.values,\n",
    "                                                                          df.label.values,\n",
    "                                                                          test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WW0v2BYtC9Q"
   },
   "outputs": [],
   "source": [
    "len(train_reviews), len(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99bOmv0rTs5p"
   },
   "source": [
    "## Text Wrangling and Normalization\n",
    "\n",
    "In this section, we will also normalize our corpus by removing accented characters, newline characters and so on. Lets get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nB7gcz5UdffC"
   },
   "source": [
    "### **Question 1**: **Complete** the following utility functions (2 points)\n",
    "\n",
    "__Hint:__ Use the knowledge gained from NLP-1 or the classification tutorial to solve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FmhycTAcTf3w"
   },
   "outputs": [],
   "source": [
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    # hint use beautifulsoup to remove html tags\n",
    "    <YOUR CODE HERE>\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    # hint use the normalize function from unicodedata\n",
    "    <YOUR CODE HERE>\n",
    "\n",
    "def pre_process_corpus(docs):\n",
    "    norm_docs = []\n",
    "    for doc in tqdm(docs):\n",
    "        # strip HTML tags\n",
    "        doc = <YOUR CODE HERE>\n",
    "        # remove extra newlines\n",
    "        doc = <YOUR CODE HERE>\n",
    "        # lower case\n",
    "        doc = <YOUR CODE HERE>\n",
    "        # remove accented characters\n",
    "        doc = <YOUR CODE HERE>\n",
    "        # fix contractions\n",
    "        doc = <YOUR CODE HERE>\n",
    "        # remove special characters\\whitespaces\n",
    "        # use regex to keep only letters, numbers and spaces\n",
    "        doc = <YOUR CODE HERE>\n",
    "        # use regex to remove extra spaces\n",
    "        doc = <YOUR CODE HERE>\n",
    "        # remove trailing and leading spaces\n",
    "        doc = <YOUR CODE HERE>\n",
    "\n",
    "        norm_docs.append(doc)\n",
    "  \n",
    "    return norm_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccste3GtTf08"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "norm_train_reviews = pre_process_corpus(train_reviews)\n",
    "norm_test_reviews = pre_process_corpus(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKKxB6MqT3DW"
   },
   "source": [
    "## Label Encode Class Labels\n",
    "\n",
    "Our dataset has labels in the form of positive and negative classes. We transform them into consumable form by performing label encoding. Label encoding assigns a unique numerical value to each class. For example: \n",
    "``negative: 0 and positive:1``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--bbxUmyT7qG"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Activation, Dense\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYNNxRtGdnxm"
   },
   "source": [
    "### **Question 2**: **Complete** the following transformations (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMLYFYekTfyF"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "# tokenize train reviews & encode train labels\n",
    "tokenized_train = <YOUR CODE HERE>\n",
    "y_train = <YOUR CODE HERE>\n",
    "# tokenize test reviews & encode test labels\n",
    "tokenized_test = <YOUR CODE HERE>\n",
    "y_test = <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvMPHqUhUCxE"
   },
   "source": [
    "## Feature Engineering based on Word2Vec Embeddings\n",
    "\n",
    "In the previous notebook we discussed different word embedding techniques like word2vec, glove, fastText, etc. In this section we will leverage ``gensim`` to transform our dataset into word2vec  representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWaE8tMjTx9M"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpQPa5Bid3jB"
   },
   "source": [
    "### **Question 3**: **Get** feature vectors using Word2Vec (2 points)\n",
    "\n",
    "Build the word2vec model on your tokenized train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wRI5lPhTx7B"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# build word2vec model\n",
    "w2v_num_features = 300\n",
    "# use a similar config as the tutorial but use a min_count of 2 and train for 10 iterations\n",
    "w2v_model = <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dfG3GqOUMZt"
   },
   "source": [
    "## Averaged Document Vectors\n",
    "\n",
    "A sentence in very simple terms is a collection of words. By now we know how to transform words into vector representation. But how do we transform sentences and documents into vector representation?\n",
    "\n",
    "A simple and na√Øve way is to average all words in a given sentence to form a sentence vector. In this section, we will leverage this technique itself to prepare our sentence/document vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llYpaGCUs6Kp"
   },
   "source": [
    "### **Question 4**: **Complete** the following utility to build a function to generate and obtain averaged document embeddings (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_hYGOxJTx4S"
   },
   "outputs": [],
   "source": [
    "def averaged_doc_vectorizer(corpus, model, num_features):\n",
    "    <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFb8NigUTx1l"
   },
   "outputs": [],
   "source": [
    "# generate averaged word vector features from word2vec model\n",
    "avg_w2v_train_features = <YOUR CODE HERE>\n",
    "avg_w2v_test_features = <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJ0ivNX6URz6"
   },
   "outputs": [],
   "source": [
    "print('Word2Vec model:> Train features shape:', avg_w2v_train_features.shape, \n",
    "      ' Test features shape:', avg_w2v_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FepcmBm4Udqq"
   },
   "source": [
    "## Define DNN Model\n",
    "\n",
    "Let us leverage ``tensorflow.keras`` to build our deep neural network for movie review classification task.\n",
    "We will make use of ``Dense`` layers with ``ReLU`` activation and ``Dropout`` to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOWow1aAeLBf"
   },
   "source": [
    "### **Question 5**: **Complete** the following utility to build a deep neural network for classification task (3 points)\n",
    "\n",
    "Use a similar architecture as the tutorial, key components listed below for reference:\n",
    "\n",
    "- 3 Dense Layers\n",
    "- 512 - 256 - 256 (neurons)\n",
    "- 20% dropout in each layer\n",
    "- 1 output layer for binary classification\n",
    "- binary crossentropy loss \n",
    "- adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahPQCuQRURxI"
   },
   "outputs": [],
   "source": [
    "def construct_deepnn_architecture(num_input_features):\n",
    "    <YOUR CODE HERE>\n",
    "    return dnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rn4ISD4UjZO"
   },
   "source": [
    "## Compile and Visualize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g39TIs2-URuz"
   },
   "outputs": [],
   "source": [
    "w2v_dnn = construct_deepnn_architecture(num_input_features=w2v_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uI9ScMZwURr9"
   },
   "outputs": [],
   "source": [
    "w2v_dnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdDbvOuoUqdM"
   },
   "source": [
    "## Train the Model using Word2Vec Features\n",
    "\n",
    "The first exercise is to leverage word2vec features as input to our deep neural network to perform moview review classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8rvfZLseVsI"
   },
   "source": [
    "### **Question 6**: **Train** the model (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfTB6mqVURpB"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "w2v_dnn.<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KN-btNFUuxX"
   },
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSAIE0_3nMYE"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zrs3GX0lUqDL"
   },
   "outputs": [],
   "source": [
    "y_pred = w2v_dnn.predict_classes(avg_w2v_test_features)\n",
    "predictions = le.inverse_transform(y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suXs6CztelVX"
   },
   "source": [
    "### **Question 7**: **Get** evaluation results (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKixXpo0UqAw"
   },
   "outputs": [],
   "source": [
    "labels = <YOUR CODE HERE>\n",
    "# print classification report\n",
    "<YOUR CODE HERE>\n",
    "# display confusion matrix\n",
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkC5J8vos_gc"
   },
   "source": [
    "Congratulations you have built your first hate speech detection model!\n",
    "\n",
    "We will look at more complex models in the future to see if we can improve this performance given this is a pretty complex dataset \\ domain as compared to basic sentiment analysis"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise 2 - Text Classification with Dense Neural Networks.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
