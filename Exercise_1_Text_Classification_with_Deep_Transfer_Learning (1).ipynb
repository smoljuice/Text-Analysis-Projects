{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sjp7WlCGajaa"
   },
   "source": [
    "# Exercise 1 - Text Classification using Pretrained Embeddings\n",
    "\n",
    "Handling tough real-world problems in Natural Language Processing (NLP) include tackling with class imbalance and the lack of availability of enough labeled data for training. Thanks to the recent advancements in deep transfer learning in NLP, we have been able to make rapid strides in not only tackling these problems but also leverage these models for diverse downstream NLP tasks.\n",
    "\n",
    "The intent of this notebook is to look at various SOTA models in deep transfer learning for NLP with hands-on examples:\n",
    "\n",
    "- Pre-trained word embeddings for Deep Learning Models (FastText with CNNs)\n",
    "- Universal Embeddings (Sentence Encoders, NNLMs)\n",
    "\n",
    "We will take a benchmark classification dataset and train and compare the performance of these models. All examples here will be showcased using Python and leveraging the latest and best of TensorFlow 2.x.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TEQr8nEx-sRR",
    "outputId": "b22b9fff-b608-4907-ca23-887b14ae8d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec  6 21:36:19 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   33C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OQZquMCajaf"
   },
   "source": [
    "# Load Necessary Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "x8WniVpUajaf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tXp8U9ENajah",
    "outputId": "3aaf0a69-76a3-4137-bacb-aedb237b809c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Version:  2.7.0\n",
      "Eager mode:  True\n",
      "TF Hub version:  0.12.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TF Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"TF Hub version: \", hub.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IYUA5Zlajak"
   },
   "source": [
    "## Load Dataset - Hate Speech\n",
    "\n",
    "Social media unfortunately is rampant with hate speech in the form of posts and comments. This is a practical example of perhaps building an automated hate speech detection system using NLP in the form of text classification.\n",
    "\n",
    "In this notebook, we will leverage an open sourced collection of hate speech posts and comments.\n",
    "\n",
    "The dataset is available here: [kaggle](https://www.kaggle.com/usharengaraju/dynamically-generated-hate-speech-dataset) which in turn has been curated from a wider [data source for hate speech](https://hatespeechdata.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V20BvLjjajal",
    "outputId": "ab940096-7dfd-41be-d3fc-2f8c7640dcb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40623 entries, 0 to 40622\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Unnamed: 0      40623 non-null  int64  \n",
      " 1   id              40623 non-null  object \n",
      " 2   text            40623 non-null  object \n",
      " 3   label           40623 non-null  object \n",
      " 4   type            40623 non-null  object \n",
      " 5   model_wrong     26097 non-null  object \n",
      " 6   db.model_preds  26097 non-null  float64\n",
      " 7   status          40623 non-null  object \n",
      " 8   round           40623 non-null  object \n",
      " 9   split           40623 non-null  object \n",
      " 10  annotator       40623 non-null  object \n",
      "dtypes: float64(1), int64(1), object(9)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('HateDataset.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ZDDJCvjrajan",
    "outputId": "5fa8e7f5-7dbd-4765-90d6-3f85f4ee3f1b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It should be legal for a husband to rape his wife</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dalits are lowlife scum</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dalits are lowlives</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It was a better world when women dare not ques...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't work this hard so that those immigrant...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  It should be legal for a husband to rape his wife  hate\n",
       "1                            dalits are lowlife scum  hate\n",
       "2                                dalits are lowlives  hate\n",
       "3  It was a better world when women dare not ques...  hate\n",
       "4  I don't work this hard so that those immigrant...  hate"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['text', 'label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdMiB1EFajap"
   },
   "source": [
    "# Preparing Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BiHTDHD_ajap"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_reviews, test_reviews, train_labels, test_labels = train_test_split(df.text.values,\n",
    "                                                                          df.label.values,\n",
    "                                                                          test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28dKBiUrO9ua",
    "outputId": "74077061-741d-4420-8f62-1491feaf0140"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32498, 8125)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_reviews), len(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXDBz_qqajas"
   },
   "source": [
    "# Basic Text Pre-processing\n",
    "\n",
    "We do minimal text pre-processing here since we are using deep learning models and not count-based methods. Steps include the following:\n",
    "\n",
    "- Removing HTML characters\n",
    "- Converting accented characters\n",
    "- Fixing contractions\n",
    "- Removing special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H-xMIu2Najas",
    "outputId": "6070b418-5c24-4072-d6ba-49d164f46948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.0.58-py2.py3-none-any.whl (8.0 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 5.3 MB/s \n",
      "\u001b[?25hCollecting pyahocorasick\n",
      "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
      "\u001b[K     |████████████████████████████████| 321 kB 49.6 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
      "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85443 sha256=7135253b82c0db9e0b56a46786ad104c47e81b4864b7e6e389fe5c617f5f3f72\n",
      "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
      "Successfully built pyahocorasick\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.0 contractions-0.0.58 pyahocorasick-1.4.2 textsearch-0.0.21\n",
      "Requirement already satisfied: textsearch in /usr/local/lib/python3.7/dist-packages (0.0.21)\n",
      "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch) (0.3.0)\n",
      "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!sudo pip3 install contractions\n",
    "!sudo pip3 install textsearch\n",
    "!sudo pip3 install tqdm\n",
    "!sudo pip3 install nltk\n",
    "!sudo pip3 install beautifulsoup4\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcAsV316Ar8X"
   },
   "source": [
    "# **Question 1**: Build the text pre-processing pipeline (3 points)\n",
    "\n",
    "__Hint:__ You can follow the same sequence of steps like the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0le4IFlxajau"
   },
   "outputs": [],
   "source": [
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def pre_process_corpus(docs):\n",
    "    norm_docs = []\n",
    "    for doc in tqdm.tqdm(docs):\n",
    "        doc = strip_html_tags(doc)\n",
    "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "        doc = doc.lower()\n",
    "        doc = remove_accented_chars(doc)\n",
    "        doc = contractions.fix(doc)\n",
    "        doc = re.sub(r'[^a-zA-Z0-9\\s]', ' ', doc, flags=re.I|re.A)\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        doc = doc.strip()  \n",
    "        norm_docs.append(doc)\n",
    "    return norm_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCa_kwSoajaw",
    "outputId": "ff2453fb-c89c-4831-f627-3d73cf767cf2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4142/32498 [00:00<00:05, 5561.69it/s]/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "100%|██████████| 32498/32498 [00:05<00:00, 5657.15it/s]\n",
      "100%|██████████| 8125/8125 [00:01<00:00, 5787.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.58 s, sys: 655 ms, total: 7.24 s\n",
      "Wall time: 7.17 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "norm_train_reviews = pre_process_corpus(train_reviews)\n",
    "norm_test_reviews = pre_process_corpus(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0Dpkx4jPlKO"
   },
   "source": [
    "## Label Encode Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxE_7E5fBCWg"
   },
   "source": [
    "# **Question 2**: Label Encode Class Labels (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PaO4N2K9PoFv"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# positive -> 1, negative -> 0\n",
    "\n",
    "le = LabelEncoder()\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xLKdlUs-PxRD"
   },
   "outputs": [],
   "source": [
    "y_train = le.fit_transform(train_labels)\n",
    "y_test = le.transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAJy5oNXcWzx"
   },
   "source": [
    "# __Question 3:__ Build Model 0 - Simple Baseline ML Model - Logistic Regression (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orFz4oyrhRWz"
   },
   "source": [
    "## Feature Extraction with BOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CKxZvOccDl7",
    "outputId": "0cac8190-3ead-4e20-b0bf-47ec3f4ec70b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW model:> Train features shape: (32498, 13076)  Test features shape: (8125, 13076)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(binary=False, min_df=2, max_df=1.0)\n",
    "\n",
    "cv_train_features = cv.fit_transform(norm_train_reviews)\n",
    "cv_test_features = cv.transform(norm_test_reviews)\n",
    "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTJzeCmAhVqf"
   },
   "source": [
    "## Train the ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZGK-qpVFcgAk",
    "outputId": "6b04cbe5-93b8-4ed0-a2d5-0ff611ac9762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.47 s, sys: 5.52 s, total: 9.99 s\n",
      "Wall time: 5.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Logistic Regression model on BOW features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate model\n",
    "lr = LogisticRegression(penalty='l2', max_iter=500, C=1, solver='lbfgs', random_state=42)\n",
    "\n",
    "# train model\n",
    "lr.fit(cv_train_features, y_train)\n",
    "\n",
    "# predict on test data\n",
    "lr_bow_predictions = lr.predict(cv_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruF1eMNLhXp5"
   },
   "source": [
    "## Predict and Test Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "XKcDYnNsco15",
    "outputId": "4df68e3d-c105-45d7-f32d-ea0c481cf093"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.77      0.76      4401\n",
      "           1       0.72      0.68      0.70      3724\n",
      "\n",
      "    accuracy                           0.73      8125\n",
      "   macro avg       0.73      0.73      0.73      8125\n",
      "weighted avg       0.73      0.73      0.73      8125\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3405</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1204</td>\n",
       "      <td>2520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  3405   996\n",
       "1  1204  2520"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(y_test, lr_bow_predictions))\n",
    "pd.DataFrame(confusion_matrix(y_test, lr_bow_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beXKvI1QARmW"
   },
   "source": [
    "___Not that great a performance! Can we do better?___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhKn8guEajay"
   },
   "source": [
    "# __Question 4:__ Build Model 1: FastText Embeddings + CNN (4 points)\n",
    "\n",
    "![](https://i.imgur.com/6Pk3Nrv.png)\n",
    "\n",
    "Convolutional Neural Networks (CNNs) have also proven to be very effective in text classification besides computer vision tasks. The idea is to leverage embeddings as features for text data and apply convolutions and poolings on them.\n",
    "\n",
    "We will leverage the ``tensorflow.keras`` utilities to tokenize text before we use the FastText embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMCYxEQqajaz"
   },
   "source": [
    "## Tokenizing text to create vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lwnUNVSBQSy"
   },
   "source": [
    "### Tokenize text corpus.\n",
    "_Hint: Use ``tf.keras.preprocessing.text.Tokenizer``_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "cj0KZG_2ajaz"
   },
   "outputs": [],
   "source": [
    "t = tf.keras.preprocessing.text.Tokenizer(oov_token='<UNK>')\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(norm_train_reviews)\n",
    "t.word_index['<PAD>'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2hvcxYvaja4"
   },
   "source": [
    "## Convert texts (sequences of words) to sequence of numeric ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kfCTE0AKaja4"
   },
   "outputs": [],
   "source": [
    "train_sequences = t.texts_to_sequences(norm_train_reviews)\n",
    "test_sequences = t.texts_to_sequences(norm_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Rhom3wvaja6",
    "outputId": "ef975e5f-07c3-48bc-f390-4c93dac9f1aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size=19098\n",
      "Number of Documents=32498\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size={}\".format(len(t.word_index)))\n",
    "print(\"Number of Documents={}\".format(t.document_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeEoOXqdaja8"
   },
   "source": [
    "## Visualizing sentence length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "QByCrNcDaja8",
    "outputId": "a3c13aef-1597-4118-f168-f5803c50eae1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 200.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(doc.split()) for doc in norm_train_reviews], bins=30);\n",
    "plt.xlim([0, 200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWrzX3eYaja-"
   },
   "source": [
    "## Padding text sequences\n",
    "\n",
    "We standardize the sentence lengths by defining a maximum length. Sentences longer than this are truncated while shorter ones are padded.\n",
    "\n",
    "___Use a max sequence length of around 250 based on the above histogram___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d_ALlOn6aja_",
    "outputId": "af8fd2e1-7e9b-47b5-b5bb-ba62980b5fb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32498, 250), (8125, 250))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "# pad dataset to a maximum review length in words\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rctc4SDbajbB"
   },
   "source": [
    "## Building FastText based Embedding Matrix\n",
    "\n",
    "Here we will build an embedding matrix based on pre-trained FastText Embeddings available __[here](https://fasttext.cc/docs/en/english-vectors.html)__.\n",
    "\n",
    "We will be using the __wiki-news-300d-1M.vec.zip__ embedding file which has 1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).\n",
    "\n",
    "![](https://i.imgur.com/5de9N5R.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NUo94CZhhLW"
   },
   "source": [
    "## Download Pre-trained FastText Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZP8QYOCA8kg"
   },
   "source": [
    "We have chosen a slightly less powerful model which should download faster than the tutorial but feel free to play around with different pretrained embeddings from [here](https://fasttext.cc/docs/en/english-vectors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUVQNz32dOnD",
    "outputId": "e89321c9-7399-4466-fe04-61d1911f734a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-06 21:37:01--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 681808098 (650M) [application/zip]\n",
      "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
      "\n",
      "wiki-news-300d-1M.v 100%[===================>] 650.22M  12.2MB/s    in 55s     \n",
      "\n",
      "2021-12-06 21:37:57 (11.8 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKLwq0nFdWEb",
    "outputId": "6f6e4cf1-7f05-421f-b608-418ea4fb2e90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  wiki-news-300d-1M.vec.zip\n",
      "  inflating: wiki-news-300d-1M.vec   \n"
     ]
    }
   ],
   "source": [
    "!unzip wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSeBiAiqhlR_"
   },
   "source": [
    "## Generate Pre-trained Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "3FatWEAzF34n"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(t.word_index)\n",
    "EMBED_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Mz5A60JEajbD"
   },
   "outputs": [],
   "source": [
    "word2idx = t.word_index\n",
    "FASTTEXT_INIT_EMBEDDINGS_FILE = './wiki-news-300d-1M.vec'\n",
    "\n",
    "\n",
    "def load_pretrained_embeddings(word_to_index, max_features, embedding_size, embedding_file_path):  \n",
    "    \"\"\"\n",
    "    Utility function to load the pre-trained embeddings\n",
    "    \"\"\"  \n",
    "    \n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*row.split(\" \")) \n",
    "                                for row in open(embedding_file_path, encoding=\"utf8\", errors='ignore') \n",
    "                                    if len(row)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    nb_words = min(max_features, len(word_to_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_size))\n",
    "    \n",
    "    for word, idx in word_to_index.items():\n",
    "        if idx >= max_features: \n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0WfOQV3lajbF",
    "outputId": "e835485d-4a4b-4733-a0c3-0e67c445f74f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19098, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get FastText embeddings based on our word to index mapping dictionary\n",
    "ft_embeddings = load_pretrained_embeddings(word_to_index=word2idx, \n",
    "                                           max_features=VOCAB_SIZE, \n",
    "                                           embedding_size=EMBED_SIZE, \n",
    "                                           embedding_file_path=FASTTEXT_INIT_EMBEDDINGS_FILE)\n",
    "ft_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEV8ZEN8ajbH"
   },
   "source": [
    "## Build Model Architecture\n",
    "\n",
    "We will use the ``tensorflow.keras`` high level API for building our deep neural network. One slight modification is required for the ``Embedding`` layer. In place of initializing this layer with random weights (as is usual), we start from FastText embeddings weights by setting the ``weights`` parameter. We also keep ``trainable`` parameter as ``True`` in order to learn/improve the pretrained weights as per our corpus. The rest of the model has usual ``Conv1D`` and ``MaxPool`` layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdwiN5AvBh-_"
   },
   "source": [
    "### Build a 1D-Convolution based classification model. Initialize the embedding layer with FastText weights\n",
    "\n",
    "___You can use a similar architecture as the tutorial or build your own!___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gbpb7gd3ajbH",
    "outputId": "8773a96d-50fe-4526-89f8-5f42d790e363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 250, 300)          5729400   \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 250, 256)          307456    \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 125, 256)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 125, 128)          131200    \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 62, 128)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 62, 64)            32832     \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 31, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1984)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               508160    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,775,097\n",
      "Trainable params: 6,775,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Embedding(VOCAB_SIZE, EMBED_SIZE,\n",
    "                                    weights=[ft_embeddings],\n",
    "                                    trainable=True,\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "model.add(tf.keras.layers.Conv1D(filters=256, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgU-nHZ2ajbJ"
   },
   "source": [
    "## Train and Validate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSfxfmWKB2yR"
   },
   "source": [
    "### Train the Model\n",
    "\n",
    "Use a similar methodology as the tutorial but use the following configs also:\n",
    "- __`validation_split`__ of __0.02__ i.e. 2%\n",
    "- 5 epochs\n",
    "- 128 batch size\n",
    "- no callbacks needed to keep things simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IpegUjivajbJ",
    "outputId": "7c3cb4be-da8e-4666-85ef-7b427d2e03e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-0cef3bddb968>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m           verbose=1)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: 2 root error(s) found.\n  (0) UNIMPLEMENTED:  Cast string to float is not supported\n\t [[node binary_crossentropy/Cast\n (defined at /usr/local/lib/python3.7/dist-packages/keras/losses.py:1797)\n]]\n  (1) CANCELLED:  Function was cancelled before it was started\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_2811]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node binary_crossentropy/Cast:\nIn[0] ExpandDims (defined at /usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py:677)\n\nOperation defined at: (most recent call last)\n>>>   File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n>>>     handler_func(fileobj, events)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n>>>     self._handle_recv()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n>>>     self._run_callback(callback, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n>>>     callback(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n>>>     return self.dispatch_shell(stream, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n>>>     handler(stream, idents, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n>>>     user_expressions, allow_stdin)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n>>>     interactivity=interactivity, compiler=compiler, result=result)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n>>>     if self.run_code(code, result):\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-31-0cef3bddb968>\", line 11, in <module>\n>>>     verbose=1)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 810, in train_step\n>>>     y, y_pred, sample_weight, regularization_losses=self.losses)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n>>>     loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 141, in __call__\n>>>     losses = call_fn(y_true, y_pred)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 245, in call\n>>>     return ag_fn(y_true, y_pred, **self._fn_kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1797, in binary_crossentropy\n>>>     y_true = tf.cast(y_true, y_pred.dtype)\n>>> \n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                      patience=2,\n",
    "                                      restore_best_weights=True,\n",
    "                                      verbose=1)\n",
    "\n",
    "model.fit(X_train, train_reviews,\n",
    "          epochs=5, \n",
    "          batch_size=128, \n",
    "          shuffle=True,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-8mIjSHajbL"
   },
   "source": [
    "## Model Performance Evaluation on the Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Omex_E4B7zC"
   },
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "NXJm1NEHajbL",
    "outputId": "e1bd0f72-19ae-4f8e-872a-15b13bf4b448"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-e920c814353f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_classes(X_test, batch_size=2048, verbose=0).ravel()\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(test_reviews, predictions)*100))\n",
    "print(classification_report(test_reviews, predictions))\n",
    "pd.DataFrame(confusion_matrix(test_reviews, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKFT6X21o1Wf"
   },
   "source": [
    "___Do you observe a better performance?___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHgcPoPoajb-"
   },
   "source": [
    "# __Question 5:__ Build Model 2: Neural Network Language Model (4 points)\n",
    "\n",
    "Authors Bengio et. al. in their paper titled [A Neural Probabilistic Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) present a novel method to learn the joint probability function of sequences of\n",
    "words in a language. This LM learns useful representation of sentences and words which can be leveraged for other NLP tasks such as Classication, Translation, etc.\n",
    "\n",
    "Let us leverage NNLM embeddings to train a classifier on the hate speech dataset\n",
    "\n",
    "![](https://i.imgur.com/blaLxUp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpIVtIJ6ajb_"
   },
   "source": [
    "## Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "jcXeFfnlFDNf"
   },
   "outputs": [],
   "source": [
    "norm_train_reviews = np.array(norm_train_reviews)\n",
    "norm_test_reviews = np.array(norm_test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f94Uo3GNajcC"
   },
   "source": [
    "## Build a NNLM Embedding Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "cvwlBRF9ajcD"
   },
   "outputs": [],
   "source": [
    "model = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\"\n",
    "hub_layer = hub.KerasLayer(model, output_shape=[128], input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tH3T6A_PajcF"
   },
   "source": [
    "## Build Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idbNviTuCCtX"
   },
   "source": [
    "### Build a Classification Model using the TF_Hub pretrained model\n",
    "\n",
    "___Use a similar architecture as the tutorial or try your own!___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5QlogyoajcG",
    "outputId": "d131764e-02e0-41b5-ba4f-308d3346cd86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 128)               124642688 \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124,675,841\n",
      "Trainable params: 124,675,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(hub_layer)\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5MprbzDajcH"
   },
   "source": [
    "## Train and Validate Model\n",
    "\n",
    "Use a similar methodology as the tutorial but use the following configs also:\n",
    "- __`validation_split`__ of __0.02__ i.e. 2%\n",
    "- 5 epochs\n",
    "- 128 batch size\n",
    "- no callbacks needed to keep things simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 869
    },
    "id": "6PnrvpPlajcJ",
    "outputId": "9eee064a-29e8-48b1-ce76-715d4827ade8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Model was constructed with shape (None,) for input KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.string, name='keras_layer_1_input'), name='keras_layer_1_input', description=\"created by layer 'keras_layer_1_input'\"), but it was called on an input with incompatible shape (None, 250).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None,) for input KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.string, name='keras_layer_1_input'), name='keras_layer_1_input', description=\"created by layer 'keras_layer_1_input'\"), but it was called on an input with incompatible shape (None, 250).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-f039e1a67f0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m           verbose=1)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"keras_layer_1\" (type KerasLayer).\n    \n    in user code:\n    \n        File \"/usr/local/lib/python3.7/dist-packages/tensorflow_hub/keras_layer.py\", line 229, in call  *\n            result = f()\n    \n        ValueError: Shape must be rank 1 but is rank 2 for '{{node text_preprocessor/tokenize/StringSplit/StringSplit}} = StringSplit[skip_empty=true](text_preprocessor/StaticRegexReplace_1, text_preprocessor/tokenize/StringSplit/Const)' with input shapes: [?,250], [].\n    \n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, 250), dtype=string)\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                      patience=3,\n",
    "                                      restore_best_weights=True,\n",
    "                                      verbose=1)\n",
    "    \n",
    "model.fit(np.array(X_train), train_reviews, \n",
    "          epochs=5, \n",
    "          batch_size=128, \n",
    "          shuffle=True,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N68uqUGGajcK"
   },
   "source": [
    "## Model Performance Evaluation on the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "x008skIhajcK",
    "outputId": "243f7def-01f0-45b4-f25c-c42dd0782932"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-f698610306a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_classes(np.array(X_train), batch_size=512, verbose=0).ravel()\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(test_reviews, predictions)*100))\n",
    "print(classification_report(test_reviews, predictions))\n",
    "pd.DataFrame(confusion_matrix(test_reviews, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBP41pQ2ajbo"
   },
   "source": [
    "# __Question 6:__ Build Model 3: Google's Universal Sentence Encoder (4 points)\n",
    "\n",
    "These models take as input English strings and\n",
    "produce as output a fixed dimensional embedding\n",
    "representation of the string.\n",
    "\n",
    "It has two models for encoding\n",
    "sentences into embedding vectors. \n",
    "- One makes use\n",
    "of the transformer (Vaswani et al., 2017) architecture\n",
    "- The other is formulated as a deep averaging network (DAN) (Iyyer et al., 2015)\n",
    "\n",
    "__Methodology 1: Transformers__\n",
    "\n",
    "- The transformer based sentence encoding model\n",
    "constructs sentence embeddings using the encoding sub-graph of the transformer architecture\n",
    "(Vaswani et al., 2017). \n",
    "- This sub-graph uses attention to compute context aware representations\n",
    "of words in a sentence that take into account both\n",
    "the ordering and identity of all the other words.\n",
    "- The context aware word representations are converted to a fixed length sentence encoding vector\n",
    "by computing the element-wise sum of the representations at each word position\n",
    "- The encoder takes as input a lowercased (Penn TreeBank) PTB tokenized string\n",
    "and outputs a 512 dimensional vector as the sentence embedding\n",
    "\n",
    "\n",
    "__Methodology 2: Deep Averaging Network (DAN)__\n",
    "\n",
    "- In the deep averaging network (DAN) (Iyyer et al.,\n",
    "2015) the input embeddings for words and\n",
    "bi-grams are first averaged together and then\n",
    "passed through a feedforward deep neural network\n",
    "(DNN) to produce sentence embeddings. \n",
    "- Similar to the Transformer encoder, the DAN encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional sentence embedding.\n",
    "\n",
    "__Training Methodology:__\n",
    "\n",
    "The encoding model is designed to be as general purpose as possible. This is accomplished by using multi-task learning whereby a single encoding model is used to feed multiple downstream tasks. \n",
    "\n",
    "Unsupervised training data for the sentence encoding models are drawn from a variety of web sources. The sources are Wikipedia, web news,\n",
    "web question-answer pages and discussion forums. We augment unsupervised learning with training on supervised data from the Stanford Natural Language Inference (SNLI) corpus.\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/HIeb3tY.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7D93uotFajbx"
   },
   "source": [
    "## Build a USE Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwwGuYNWCU1Y"
   },
   "source": [
    "### Using Tensorflow hub, prepare an instance of ``hub.KerasLayer`` to get sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "W1OT_A00jTHy"
   },
   "outputs": [],
   "source": [
    "model = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "hub_layer = hub.KerasLayer(model, output_shape=[512], input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYjvFE7dajb0"
   },
   "source": [
    "## Build Model Architecture\n",
    "\n",
    "___Use a similar architecture as the tutorial or try your own!___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3nWJfhdjp8w",
    "outputId": "2ebac03a-70f0-4e3b-b350-9f961d6125c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_1 (KerasLayer)  (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,995,201\n",
      "Trainable params: 256,995,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(hub_layer)\n",
    "\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjYqKLkMajb2"
   },
   "source": [
    "## Train and Validate Model\n",
    "\n",
    "Use a similar methodology as the tutorial but use the following configs also:\n",
    "- __`validation_split`__ of __0.02__ i.e. 2%\n",
    "- 5 epochs\n",
    "- 128 batch size\n",
    "- no callbacks needed to keep things simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 869
    },
    "id": "DPGdUWzFj5al",
    "outputId": "b3c241e3-bba6-4642-db6e-ce084f76e3bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Model was constructed with shape (None,) for input KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.string, name='keras_layer_1_input'), name='keras_layer_1_input', description=\"created by layer 'keras_layer_1_input'\"), but it was called on an input with incompatible shape (None, 250).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None,) for input KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.string, name='keras_layer_1_input'), name='keras_layer_1_input', description=\"created by layer 'keras_layer_1_input'\"), but it was called on an input with incompatible shape (None, 250).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-f039e1a67f0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m           verbose=1)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"keras_layer_1\" (type KerasLayer).\n    \n    in user code:\n    \n        File \"/usr/local/lib/python3.7/dist-packages/tensorflow_hub/keras_layer.py\", line 229, in call  *\n            result = f()\n    \n        ValueError: Shape must be rank 1 but is rank 2 for '{{node text_preprocessor/tokenize/StringSplit/StringSplit}} = StringSplit[skip_empty=true](text_preprocessor/StaticRegexReplace_1, text_preprocessor/tokenize/StringSplit/Const)' with input shapes: [?,250], [].\n    \n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, 250), dtype=string)\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                      patience=3,\n",
    "                                      restore_best_weights=True,\n",
    "                                      verbose=1)\n",
    "    \n",
    "model.fit(np.array(X_train), train_reviews, \n",
    "          epochs=5, \n",
    "          batch_size=128, \n",
    "          shuffle=True,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmPBH7Tfajb8"
   },
   "source": [
    "## Model Performance Evaluation on the Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSWps6DsgqoO"
   },
   "source": [
    "### **Question 7**: Get Evaluation Results of the model (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "febzhocgknWs",
    "outputId": "704f82dc-8bf4-42ff-9564-0acf05cfa794"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f698610306a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_classes(np.array(X_train), batch_size=512, verbose=0).ravel()\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(test_reviews, predictions)*100))\n",
    "print(classification_report(test_reviews, predictions))\n",
    "pd.DataFrame(confusion_matrix(test_reviews, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krdIWpzXve9D"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise_1_Text_Classification_with_Deep_Transfer_Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
