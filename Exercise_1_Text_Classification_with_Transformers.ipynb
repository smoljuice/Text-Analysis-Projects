{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Exercise 1 - Text Classification with Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjp7WlCGajaa"
      },
      "source": [
        "# Deep Transfer Learning for Text Classification\n",
        "\n",
        "Handling tough real-world problems in Natural Language Processing (NLP) include tackling with class imbalance and the lack of availability of enough labeled data for training. Thanks to the recent advancements in deep transfer learning in NLP, we have been able to make rapid strides in not only tackling these problems but also leverage these models for diverse downstream NLP tasks.\n",
        "\n",
        "In this exercise you will be writing code to build a Hate Speech Classifier using:\n",
        "\n",
        "- BERT (fine-tuning)\n",
        "- DistilBERT (fine-tuning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBtXjYJHajab"
      },
      "source": [
        "# GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro9zTCDIajab"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OQZquMCajaf"
      },
      "source": [
        "# Load Necessary Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q55jt2QJbSiC"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QORPi59-V8Jk"
      },
      "source": [
        "import transformers\n",
        "transformers.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8WniVpUajaf"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import transformers\n",
        "import tqdm\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXp8U9ENajah"
      },
      "source": [
        "print(\"TF Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"TF Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IYUA5Zlajak"
      },
      "source": [
        "## Load Dataset - Hate Speech\n",
        "\n",
        "Social media unfortunately is rampant with hate speech in the form of posts and comments. This is a practical example of perhaps building an automated hate speech detection system using NLP in the form of text classification.\n",
        "\n",
        "In this notebook, we will leverage an open sourced collection of hate speech posts and comments.\n",
        "\n",
        "The dataset is available here: [kaggle](https://www.kaggle.com/usharengaraju/dynamically-generated-hate-speech-dataset) which in turn has been curated from a wider [data source for hate speech](https://hatespeechdata.com/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V20BvLjjajal"
      },
      "source": [
        "df = pd.read_csv('HateDataset.csv')\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DQuqZoRS9rz"
      },
      "source": [
        "# Subset Dataset\n",
        "\n",
        "BERT is a HUGE model which takes a long time to fine-tune!  \n",
        "\n",
        "So let's try to subset our data so you can work with a small dataset and train faster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDDJCvjrajan"
      },
      "source": [
        "df = df[['text', 'label']]\n",
        "df = df.sample(10000, random_state=42)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdMiB1EFajap"
      },
      "source": [
        "# Preparing Train, Validation and Test Datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ergvVKSUxhj"
      },
      "source": [
        "### Prepare Train-Test Split. Keep test set to be 20% of the total"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiHTDHD_ajap"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_reviews, test_reviews, train_labels, test_labels = train_test_split(df.text.values,\n",
        "                                                                          df.label.values,\n",
        "                                                                          test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ6MU6B0ITH4"
      },
      "source": [
        "len(train_reviews), len(test_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXDBz_qqajas"
      },
      "source": [
        "# No Text Pre-processing\n",
        "\n",
        "__Note:__ For some models we don't use any pre-processing like BERT! It should be able to handle a wide variety of text in its natural format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9ZD5JPwU1sO"
      },
      "source": [
        "## Label Encode Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4G7RK_tU3t5"
      },
      "source": [
        "# **Question 1**: Label Encode Class Labels\n",
        "\n",
        "`hate` and `nothate` needs to be encoded to numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlEtGVoFIpfW"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = <YOUR CODE HERE>\n",
        "\n",
        "y_train = <YOUR CODE HERE> # fit - transform on train labels\n",
        "y_test = <YOUR CODE HERE>  # transform on test labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-YydwrQajcM"
      },
      "source": [
        "# Model 1: BERT (Bi-directional Encoder Representations from Transformers)\n",
        "\n",
        "We will be using the BERT base model which has already been pre-trained by Google on MLM + Next Sentence Prediction Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OIAAHBPajcM"
      },
      "source": [
        "## BERT Tokenization\n",
        "\n",
        "The BERT model we're using expects lowercase data. Here we leverage Huggingface `transformers`' `BertTokenizer`, which breaks words into word pieces.\n",
        "\n",
        "Word Piece Tokenizer is based on [Byte Pair Encodings (BPE)](https://www.aclweb.org/anthology/P16-1162).\n",
        "\n",
        "WordPiece and BPE are two similar and commonly used techniques to segment words into subword-level in NLP tasks. In both cases, the vocabulary is initialized with all the individual characters in the language, and then the most frequent/likely combinations of the symbols in the vocabulary are iteratively added to the vocabulary.\n",
        "\n",
        "magine that the model sees the word walking. Unless this word occurs at least a few times in the training corpus, the model can't learn to deal with this word very well. However, it may have the words walked, walker, walks, each occurring only a few times. Without subword segmentation, all these words are treated as completely different words by the model.\n",
        "\n",
        "However, if these get segmented as walk@@ ing, walk@@ ed, etc., notice that all of them will now have walk@@ in common, which will occur much frequently while training, and the model might be able to learn more about it.\n",
        "\n",
        "Huggingface's transformers library has easy to use utilities for each type of model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cuHh91_U-RZ"
      },
      "source": [
        "# **Question 2**: Load BERT Tokenizer\n",
        "\n",
        "_Hint: Use ``transformers.BertTokenizer.from_pretrained`` with the right pretrained model (bert base uncased) similar to the tutorial_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1BtlCzfajcM"
      },
      "source": [
        "tokenizer = <YOUR CODE HERE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EKS_rEpajcN"
      },
      "source": [
        "## BERT Data Preparation\n",
        "\n",
        "We need to preprocess our data so that it matches the data format BERT was trained on. For this, we'll need to do a couple of things.\n",
        "\n",
        "- Lowercase our text (if we're using a BERT lowercase model)\n",
        "- Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
        "- Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
        "- Map our words to indexes using a vocab file that BERT provides\n",
        "- Add special \"CLS\" and \"SEP\" tokens (see the readme)\n",
        "- Append \"mask\" and \"segment\" tokens to each input (see the BERT paper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szOzzJUQa8hJ"
      },
      "source": [
        "# **Question 3**: Create function to tokenize and encode text into BERT token IDs\n",
        "\n",
        "Use a similar strategy as you learnt in the tutorial and fill in the following function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAEd7xjIajcO"
      },
      "source": [
        "def create_bert_input_features(tokenizer, docs, max_seq_length):\n",
        "    \n",
        "    <YOUR CODE HERE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkqG8EQLajcQ"
      },
      "source": [
        "# **Question 4**: Build Model Architecture\n",
        "\n",
        "Use a similar model architecture as the tutorial with a max sequence length of 500 tokens to be used.\n",
        "\n",
        "You can always experiment with aspects like learning rate, number of layers etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTEpLgzjajcQ"
      },
      "source": [
        "MAX_SEQ_LENGTH = 500\n",
        "\n",
        "<YOUR CODE HERE>\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRTMV4jMajcR"
      },
      "source": [
        "## Convert text to BERT input features\n",
        "\n",
        "We leverage our utility function we created earlier to convert our text documents into BERT input features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeUQ0WybVD2m"
      },
      "source": [
        "# **Question 5**: Prepare Feature ID, masks and segments for train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC_9wq2fajcS"
      },
      "source": [
        "train_features_ids, train_features_masks, train_features_segments = <YOUR CODE HERE>\n",
        "\n",
        "test_features_ids, test_features_masks, test_features_segments = <YOUR CODE HERE>\n",
        "\n",
        "print('Train Features:', train_features_ids.shape, train_features_masks.shape, train_features_segments.shape)\n",
        "print('Test Features:', test_features_ids.shape, test_features_masks.shape, test_features_segments.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AxRNcQ5ajcT"
      },
      "source": [
        "## Train and Validate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it9y1wnKVKji"
      },
      "source": [
        "# **Question 6**: Train the Model\n",
        "\n",
        "You can train it for around 3 epochs as it takes a fair bit of time to train even on a good GPU\n",
        "\n",
        "For validation data you can use the test data tokens from the previous cell to keep things simple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys_e6K0-ajcU"
      },
      "source": [
        "<YOUR CODE HERE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb_lPfpSajcY"
      },
      "source": [
        "# **Question 7**: Model Performance Evaluation on the Test Dataset\n",
        "\n",
        "Show the model's performance on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSuV86S0ajcY"
      },
      "source": [
        "<YOUR CODE HERE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD9ANmhoajcZ"
      },
      "source": [
        "# Model 2: DistilBERT (Distilled BERT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgsGB-UWajcZ"
      },
      "source": [
        "## BERT Tokenization\n",
        "\n",
        "The DistilBERT model we're using expects lowercase data. Here we leverage Huggingface `transformers`' `DistilBertTokenizer`, which breaks words into word pieces.\n",
        "\n",
        "Word Piece Tokenizer is based on [Byte Pair Encodings (BPE)](https://www.aclweb.org/anthology/P16-1162).\n",
        "\n",
        "WordPiece and BPE are two similar and commonly used techniques to segment words into subword-level in NLP tasks. In both cases, the vocabulary is initialized with all the individual characters in the language, and then the most frequent/likely combinations of the symbols in the vocabulary are iteratively added to the vocabulary.\n",
        "\n",
        "magine that the model sees the word walking. Unless this word occurs at least a few times in the training corpus, the model can't learn to deal with this word very well. However, it may have the words walked, walker, walks, each occurring only a few times. Without subword segmentation, all these words are treated as completely different words by the model.\n",
        "\n",
        "However, if these get segmented as walk@@ ing, walk@@ ed, etc., notice that all of them will now have walk@@ in common, which will occur much frequently while training, and the model might be able to learn more about it.\n",
        "\n",
        "Huggingface's transformers library has easy to use utilities for each type of model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eANXRl8BVPvG"
      },
      "source": [
        "# **Question 8**: Load DistilBERT Tokenizer\n",
        "\n",
        "_Hint: Use ``transformers.DistilBertTokenizer.from_pretrained`` with the right pretrained model (distilbert base uncased) similar to the tutorial_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9H4AeL_ajcZ"
      },
      "source": [
        "tokenizer = <YOUR CODE HERE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETJDWVw0ajca"
      },
      "source": [
        "## DistilBERT Data Preparation\n",
        "\n",
        "We need to preprocess our data so that it matches the data format DistilBERT was trained on. For this, we'll need to do a couple of things.\n",
        "\n",
        "- Lowercase our text (if we're using a BERT lowercase model)\n",
        "- Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
        "- Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
        "- Map our words to indexes using a vocab file that BERT provides\n",
        "- Add special \"CLS\" and \"SEP\" tokens (see the readme)\n",
        "- Append \"mask\" tokens to each input (see https://medium.com/huggingface/distilbert-8cf3380435b5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0fEfbhEk24b"
      },
      "source": [
        "# **Question 9**: Create function to tokenize and encode text into DistilBERT token IDs\n",
        "\n",
        "Use a similar strategy as you learnt in the tutorial and fill in the following function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azjnxnFRajcb"
      },
      "source": [
        "def create_bert_input_features(tokenizer, docs, max_seq_length):\n",
        "    \n",
        "    <YOUR CODE HERE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SoR1dwHajcc"
      },
      "source": [
        "# **Question 10**: Build Model Architecture\n",
        "\n",
        "Use a similar model architecture as the tutorial with a max sequence length of 500 tokens to be used.\n",
        "\n",
        "You can always experiment with aspects like learning rate, number of layers etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvdzoH7vajcc"
      },
      "source": [
        "MAX_SEQ_LENGTH = 500\n",
        "\n",
        "<YOUR CODE HERE>\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-0k79jRajce"
      },
      "source": [
        "# **Question 11**: Prepare Feature ID, masks and segments for train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEZtopKYajcf"
      },
      "source": [
        "train_features_ids, train_features_masks = <YOUR CODE HERE>\n",
        "test_features_ids, test_features_masks = <YOUR CODE HERE>\n",
        "\n",
        "print('Train Features:', train_features_ids.shape, train_features_masks.shape)\n",
        "print('Val Features:', test_features_ids.shape, test_features_masks.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q316Jw8jajcg"
      },
      "source": [
        "## Train and Validate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu06rG3MVfQI"
      },
      "source": [
        "# **Question 12**: Train the Model\n",
        "\n",
        "You can train it for around 3 epochs as it takes a fair bit of time to train even on a good GPU\n",
        "\n",
        "For validation data you can use the test data tokens from the previous cell to keep things simple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4UqJQTOajcg"
      },
      "source": [
        "<YOUR CODE HERE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcxGMWWcajcl"
      },
      "source": [
        "# **Question 13**: Model Performance Evaluation on the Test Dataset\n",
        "\n",
        "Show the model's performance on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEVSsaBBajcl"
      },
      "source": [
        "<YOUR CODE HERE>"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}