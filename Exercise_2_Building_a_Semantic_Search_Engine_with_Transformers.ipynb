{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise 2 - Building a Semantic Search Engine with Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w2MPbHppeO0"
      },
      "source": [
        "# Building a Semantic Search Engine to Search for Queries with Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb75SQP5pro_"
      },
      "source": [
        "# Semantic Search\n",
        "Semantic search seeks to improve search accuracy by understanding the content of the search query. In contrast to traditional search engines, that only finds documents based on lexical matches, semantic search can also find synonyms.\n",
        "\n",
        "\n",
        "## Background\n",
        "The idea behind semantic search is to embedd all entries in your corpus, which can be sentences, paragraphs, or documents, into a vector space. \n",
        "\n",
        "At search time, the query is embedded into the same vector space and the closest embedding from your corpus are found. These entries should have a high semantic overlap with the query.\n",
        "\n",
        "![SemanticSearch](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/SemanticSearch.png) \n",
        "\n",
        "\n",
        "## Similarity Computation\n",
        "\n",
        "For small corpora (up to about 100k entries) we can compute the cosine-similarity between the query and all entries in the corpus.\n",
        "\n",
        "For small corpora with few example sentences we compute the embeddings for the corpus as well as for our query.\n",
        "\n",
        "We then use the [util.pytorch_cos_sim()](../../../docs/usage/semantic_textual_similarity.md) function to compute the cosine similarity between the query and all corpus entries.\n",
        "\n",
        "For large corpora, sorting all scores would take too much time. Hence, we can use [torch.topk](https://pytorch.org/docs/stable/generated/torch.topk.html) to only get the top k entries.\n",
        "\n",
        "[Reference](https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search)\n",
        "\n",
        "\n",
        "## Objective\n",
        "\n",
        "For today's objective we will create a corpus of around 50000 question titles asked on Quora from an open dataset. Your task will be to compute sentence embeddings and then try to retrieve top 5 similar questions from the corpus for a few example queries mentioned below.\n",
        "\n",
        "Use [Sentence Transformers](https://github.com/UKPLab/sentence-transformers) which provides a scalable way to generate document embeddings using transformers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "637DLY-vqofj"
      },
      "source": [
        "## Load Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G6XYAOjKfoe"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4Gb28zvKmfb"
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZbso1ipKa2S"
      },
      "source": [
        "import transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRSgF-ITKWL6"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd9fnlmlqqxh"
      },
      "source": [
        "## Download and Load Corpus of Questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O37SmvVPK4Q_"
      },
      "source": [
        "!wget http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz3j4m13KtaR"
      },
      "source": [
        "df = pd.read_csv('quora_duplicate_questions.tsv', sep='\\t').head(25000)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3AfKmnGLCFg"
      },
      "source": [
        "corpus = df['question1'].tolist() + df['question2'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKfVgMPELMvT"
      },
      "source": [
        "len(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v-WQjCAqvd9"
      },
      "source": [
        "## Use Sentence Transformers and Generate Corpus Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuwhkFevq1rm"
      },
      "source": [
        "__Hint:__ You can use this tutorial as a reference\n",
        "\n",
        "[Semantic Search Tutorial](https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/semantic-search/semantic_search.py)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufWzsq6dt4vI"
      },
      "source": [
        "# __Question 1__: Load Pre-trained Embedder Model\n",
        "\n",
        "Load the __`roberta-large-nli-stsb-mean-tokens`__ model to generate embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf8Q7vV4LP3A"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJIxH70vL3L1"
      },
      "source": [
        "model = 'roberta-large-nli-stsb-mean-tokens'\n",
        "embedder = <YOUR CODE HERE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1LzJrmbuOaW"
      },
      "source": [
        "# __Question 2__: Generate Corpus Embeddings\n",
        "\n",
        "Generate embeddings for each and every document using the pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G27sHdxRPPri"
      },
      "source": [
        "corpus_embeddings = <YOUR CODE HERE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NVNIPeRP0Uj"
      },
      "source": [
        "corpus_embeddings.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJS9cPFkrJO_"
      },
      "source": [
        "# __Question 3__: Create a function to print top K similar sentences for a given query\n",
        "\n",
        "Use cosine similarity by leveraging the pytorch utility in `sentence_transformers` as depicted in the previously linked tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz4J-WCdP7uz"
      },
      "source": [
        "from sentence_transformers import util\n",
        "import torch\n",
        "\n",
        "def print_similar_sentences(query, model_embedder, corpus_embeddings, top_k):\n",
        "    \"\"\"\n",
        "      query: this should be your input query\n",
        "      model_embedder: this should be your embedding model (pre-trained model which you loaded earlier)\n",
        "      corpus_embeddings: this should hold the embeddings you generate for your corpus\n",
        "      top_k: the top k similar queries you should return\n",
        "    \"\"\"\n",
        "\n",
        "    <YOUR CODE HERE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW53grWprO73"
      },
      "source": [
        "# __Question 4__: Perform Semantic Search on Sample Questions to get Similar Queries from the Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGbnncfyQsC8"
      },
      "source": [
        "s = 'What is the step by step guide to invest'\n",
        "print_similar_sentences(query=<YOUR CODE HERE>,\n",
        "                        model_embedder=<YOUR CODE HERE>, \n",
        "                        corpus_embeddings=<YOUR CODE HERE>,\n",
        "                        top_k=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFlPlIERQ9xy"
      },
      "source": [
        "s = 'What is Data Science?'\n",
        "<YOUR CODE HERE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1Sse303SloA"
      },
      "source": [
        "s = 'What is natural language processing?'\n",
        "<YOUR CODE HERE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCaAX2L_S1FV"
      },
      "source": [
        "s = 'What is natural language processing?'\n",
        "<YOUR CODE HERE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTaLBUMpS7lc"
      },
      "source": [
        "s = 'Best Harry Potter Movie?'\n",
        "<YOUR CODE HERE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5Vs1PWCTGJA"
      },
      "source": [
        "s = 'What is the best smartphone?'\n",
        "<YOUR CODE HERE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9MxlFbsTaeT"
      },
      "source": [
        "s = 'What is the best starter pokemon?'\n",
        "<YOUR CODE HERE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5wrOme0Tqrg"
      },
      "source": [
        "s = 'Batman or Superman?'\n",
        "<YOUR CODE HERE>"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}