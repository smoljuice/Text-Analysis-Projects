{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sjp7WlCGajaa"
   },
   "source": [
    "# Exercise 1 - Text Classification using Pretrained Embeddings\n",
    "\n",
    "Handling tough real-world problems in Natural Language Processing (NLP) include tackling with class imbalance and the lack of availability of enough labeled data for training. Thanks to the recent advancements in deep transfer learning in NLP, we have been able to make rapid strides in not only tackling these problems but also leverage these models for diverse downstream NLP tasks.\n",
    "\n",
    "The intent of this notebook is to look at various SOTA models in deep transfer learning for NLP with hands-on examples:\n",
    "\n",
    "- Pre-trained word embeddings for Deep Learning Models (FastText with CNNs)\n",
    "- Universal Embeddings (Sentence Encoders, NNLMs)\n",
    "\n",
    "We will take a benchmark classification dataset and train and compare the performance of these models. All examples here will be showcased using Python and leveraging the latest and best of TensorFlow 2.x.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TEQr8nEx-sRR",
    "outputId": "3c42bb73-e2d3-40ea-d919-2282a522e084"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 26 00:41:54 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   34C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OQZquMCajaf"
   },
   "source": [
    "# Load Necessary Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "x8WniVpUajaf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tXp8U9ENajah",
    "outputId": "059a51ef-329c-4191-dcfd-4e35d2b2b2b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Version:  2.4.1\n",
      "Eager mode:  True\n",
      "TF Hub version:  0.11.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TF Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"TF Hub version: \", hub.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IYUA5Zlajak"
   },
   "source": [
    "## Load Dataset - Hate Speech\n",
    "\n",
    "Social media unfortunately is rampant with hate speech in the form of posts and comments. This is a practical example of perhaps building an automated hate speech detection system using NLP in the form of text classification.\n",
    "\n",
    "In this notebook, we will leverage an open sourced collection of hate speech posts and comments.\n",
    "\n",
    "The dataset is available here: [kaggle](https://www.kaggle.com/usharengaraju/dynamically-generated-hate-speech-dataset) which in turn has been curated from a wider [data source for hate speech](https://hatespeechdata.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V20BvLjjajal",
    "outputId": "40331167-a325-49dc-fcd6-0abba1560ce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40623 entries, 0 to 40622\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Unnamed: 0      40623 non-null  int64  \n",
      " 1   id              40623 non-null  object \n",
      " 2   text            40623 non-null  object \n",
      " 3   label           40623 non-null  object \n",
      " 4   type            40623 non-null  object \n",
      " 5   model_wrong     26097 non-null  object \n",
      " 6   db.model_preds  26097 non-null  float64\n",
      " 7   status          40623 non-null  object \n",
      " 8   round           40623 non-null  object \n",
      " 9   split           40623 non-null  object \n",
      " 10  annotator       40623 non-null  object \n",
      "dtypes: float64(1), int64(1), object(9)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('HateDataset.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "ZDDJCvjrajan",
    "outputId": "3f833c0b-0a0d-427a-ed05-bd9ec91ea122"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It should be legal for a husband to rape his wife</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dalits are lowlife scum</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dalits are lowlives</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It was a better world when women dare not ques...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't work this hard so that those immigrant...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  It should be legal for a husband to rape his wife  hate\n",
       "1                            dalits are lowlife scum  hate\n",
       "2                                dalits are lowlives  hate\n",
       "3  It was a better world when women dare not ques...  hate\n",
       "4  I don't work this hard so that those immigrant...  hate"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['text', 'label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdMiB1EFajap"
   },
   "source": [
    "# Preparing Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BiHTDHD_ajap"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_reviews, test_reviews, train_labels, test_labels = train_test_split(df.text.values,\n",
    "                                                                          df.label.values,\n",
    "                                                                          test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28dKBiUrO9ua",
    "outputId": "265dcf8c-fb95-4b27-db39-925ad2a50e24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32498, 8125)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_reviews), len(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXDBz_qqajas"
   },
   "source": [
    "# Basic Text Pre-processing\n",
    "\n",
    "We do minimal text pre-processing here since we are using deep learning models and not count-based methods. Steps include the following:\n",
    "\n",
    "- Removing HTML characters\n",
    "- Converting accented characters\n",
    "- Fixing contractions\n",
    "- Removing special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H-xMIu2Najas",
    "outputId": "88a3a8cf-0a7a-4ee4-fa8b-aa6755a22cc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading https://files.pythonhosted.org/packages/0a/04/d5e0bb9f2cef5d15616ebf68087a725c5dbdd71bd422bcfb35d709f98ce7/contractions-0.0.48-py2.py3-none-any.whl\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
      "Collecting anyascii\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/c7/61370d9e3c349478e89a5554c1e5d9658e1e3116cc4f2528f568909ebdf1/anyascii-0.1.7-py3-none-any.whl (260kB)\n",
      "\u001b[K     |████████████████████████████████| 266kB 12.9MB/s \n",
      "\u001b[?25hCollecting pyahocorasick\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/92/b3c70b8cf2b76f7e3e8b7243d6f06f7cb3bab6ada237b1bce57604c5c519/pyahocorasick-1.4.1.tar.gz (321kB)\n",
      "\u001b[K     |████████████████████████████████| 327kB 27.1MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
      "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.1-cp37-cp37m-linux_x86_64.whl size=85271 sha256=3017fc50b01e5354db7b61e6aa47416921c19a9e4dd9a4a059f50a1842893cba\n",
      "  Stored in directory: /root/.cache/pip/wheels/e4/ab/f7/cb39270df8f6126f3dd4c33d302357167086db460968cfc80c\n",
      "Successfully built pyahocorasick\n",
      "Installing collected packages: anyascii, pyahocorasick, textsearch, contractions\n",
      "Successfully installed anyascii-0.1.7 contractions-0.0.48 pyahocorasick-1.4.1 textsearch-0.0.21\n",
      "Requirement already satisfied: textsearch in /usr/local/lib/python3.7/dist-packages (0.0.21)\n",
      "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch) (0.1.7)\n",
      "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch) (1.4.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!sudo pip3 install contractions\n",
    "!sudo pip3 install textsearch\n",
    "!sudo pip3 install tqdm\n",
    "!sudo pip3 install nltk\n",
    "!sudo pip3 install beautifulsoup4\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcAsV316Ar8X"
   },
   "source": [
    "# **Question 1**: Build the text pre-processing pipeline\n",
    "\n",
    "__Hint:__ You can follow the same sequence of steps like the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0le4IFlxajau"
   },
   "outputs": [],
   "source": [
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def pre_process_corpus(docs):\n",
    "    norm_docs = []\n",
    "    for doc in tqdm.tqdm(docs):\n",
    "        doc = strip_html_tags(doc)\n",
    "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "        doc = doc.lower()\n",
    "        doc = remove_accented_chars(doc)\n",
    "        doc = contractions.fix(doc)\n",
    "        # lower case and remove special characters\\whitespaces\n",
    "        doc = re.sub(r'[^a-zA-Z0-9\\s]', ' ', doc, flags=re.I|re.A)\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        doc = doc.strip()  \n",
    "        norm_docs.append(doc)\n",
    "    return norm_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCa_kwSoajaw",
    "outputId": "6609bce4-7e26-4f76-bbfb-039e775e760c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3951/32498 [00:00<00:03, 7938.59it/s]/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "100%|██████████| 32498/32498 [00:04<00:00, 8098.70it/s]\n",
      "100%|██████████| 8125/8125 [00:01<00:00, 7985.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.75 s, sys: 390 ms, total: 5.14 s\n",
      "Wall time: 5.04 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "norm_train_reviews = pre_process_corpus(train_reviews)\n",
    "norm_test_reviews = pre_process_corpus(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0Dpkx4jPlKO"
   },
   "source": [
    "## Label Encode Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxE_7E5fBCWg"
   },
   "source": [
    "# **Question 2**: Label Encode Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PaO4N2K9PoFv"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# positive -> 1, negative -> 0\n",
    "\n",
    "le = LabelEncoder()\n",
    "num_classes=2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xLKdlUs-PxRD"
   },
   "outputs": [],
   "source": [
    "y_train = le.fit_transform(train_labels)\n",
    "y_test = le.transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAJy5oNXcWzx"
   },
   "source": [
    "# __Question 3:__ Build Model 0 - Simple Baseline ML Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orFz4oyrhRWz"
   },
   "source": [
    "## Feature Extraction with BOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CKxZvOccDl7",
    "outputId": "4081d64a-1794-44da-e5db-a7a55af7cf07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW model:> Train features shape: (32498, 13077)  Test features shape: (8125, 13077)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(binary=False, min_df=2, max_df=1.0)\n",
    "\n",
    "cv_train_features = cv.fit_transform(norm_train_reviews)\n",
    "cv_test_features = cv.transform(norm_test_reviews)\n",
    "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTJzeCmAhVqf"
   },
   "source": [
    "## Train the ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZGK-qpVFcgAk",
    "outputId": "775f867f-7bb0-45d3-878c-e58d447331ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.92 s, sys: 4.93 s, total: 8.85 s\n",
      "Wall time: 4.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Logistic Regression model on BOW features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate model\n",
    "lr = LogisticRegression(penalty='l2', max_iter=500, C=1, solver='lbfgs', random_state=42)\n",
    "\n",
    "# train model\n",
    "lr.fit(cv_train_features, y_train)\n",
    "\n",
    "# predict on test data\n",
    "lr_bow_predictions = lr.predict(cv_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruF1eMNLhXp5"
   },
   "source": [
    "## Predict and Test Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "XKcDYnNsco15",
    "outputId": "01ba778a-7899-4336-818f-35013fc84d37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.78      0.76      4401\n",
      "           1       0.72      0.68      0.70      3724\n",
      "\n",
      "    accuracy                           0.73      8125\n",
      "   macro avg       0.73      0.73      0.73      8125\n",
      "weighted avg       0.73      0.73      0.73      8125\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3416</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1208</td>\n",
       "      <td>2516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  3416   985\n",
       "1  1208  2516"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(y_test, lr_bow_predictions))\n",
    "pd.DataFrame(confusion_matrix(y_test, lr_bow_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beXKvI1QARmW"
   },
   "source": [
    "___Not that great a performance! Can we do better?___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhKn8guEajay"
   },
   "source": [
    "# __Question 4:__ Build Model 1: FastText Embeddings + CNN \n",
    "\n",
    "![](https://i.imgur.com/6Pk3Nrv.png)\n",
    "\n",
    "Convolutional Neural Networks (CNNs) have also proven to be very effective in text classification besides computer vision tasks. The idea is to leverage embeddings as features for text data and apply convolutions and poolings on them.\n",
    "\n",
    "We will leverage the ``tensorflow.keras`` utilities to tokenize text before we use the FastText embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMCYxEQqajaz"
   },
   "source": [
    "## Tokenizing text to create vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lwnUNVSBQSy"
   },
   "source": [
    "### Tokenize text corpus.\n",
    "_Hint: Use ``tf.keras.preprocessing.text.Tokenizer``_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "cj0KZG_2ajaz"
   },
   "outputs": [],
   "source": [
    "t = tf.keras.preprocessing.text.Tokenizer(oov_token='<UNK>')\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(norm_train_reviews)\n",
    "t.word_index['<PAD>'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2hvcxYvaja4"
   },
   "source": [
    "## Convert texts (sequences of words) to sequence of numeric ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "kfCTE0AKaja4"
   },
   "outputs": [],
   "source": [
    "train_sequences = t.texts_to_sequences(norm_train_reviews)\n",
    "test_sequences = t.texts_to_sequences(norm_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Rhom3wvaja6",
    "outputId": "303b4945-787b-4949-b1dc-050ce8bafa25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size=19098\n",
      "Number of Documents=32498\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size={}\".format(len(t.word_index)))\n",
    "print(\"Number of Documents={}\".format(t.document_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeEoOXqdaja8"
   },
   "source": [
    "## Visualizing sentence length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "QByCrNcDaja8",
    "outputId": "50d0175b-24c0-406e-a03a-b550039bb38b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 2000.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(doc.split()) for doc in norm_train_reviews], bins=30);\n",
    "plt.xlim([0, 2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWrzX3eYaja-"
   },
   "source": [
    "## Padding text sequences\n",
    "\n",
    "We standardize the sentence lengths by defining a maximum length. Sentences longer than this are truncated while shorter ones are padded.\n",
    "\n",
    "___Use a max sequence length of around 250 based on the above histogram___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d_ALlOn6aja_",
    "outputId": "5d152f63-607d-4cba-bcb0-e72b54e54832"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32498, 250), (8125, 250))"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "# pad dataset to a maximum review length in words\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rctc4SDbajbB"
   },
   "source": [
    "## Building FastText based Embedding Matrix\n",
    "\n",
    "Here we will build an embedding matrix based on pre-trained FastText Embeddings available __[here](https://fasttext.cc/docs/en/english-vectors.html)__.\n",
    "\n",
    "We will be using the __wiki-news-300d-1M.vec.zip__ embedding file which has 1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).\n",
    "\n",
    "![](https://i.imgur.com/5de9N5R.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NUo94CZhhLW"
   },
   "source": [
    "## Download Pre-trained FastText Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZP8QYOCA8kg"
   },
   "source": [
    "We have chosen a slightly less powerful model which should download faster than the tutorial but feel free to play around with different pretrained embeddings from [here](https://fasttext.cc/docs/en/english-vectors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUVQNz32dOnD",
    "outputId": "8c36b8db-0b5b-45db-b8c7-5307cf53bd47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-26 00:51:29--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 681808098 (650M) [application/zip]\n",
      "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
      "\n",
      "wiki-news-300d-1M.v 100%[===================>] 650.22M  4.57MB/s    in 2m 4s   \n",
      "\n",
      "2021-03-26 00:53:34 (5.23 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKLwq0nFdWEb",
    "outputId": "37e8c5ed-32c3-4e0d-96be-d0a78c5eb737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  wiki-news-300d-1M.vec.zip\n",
      "  inflating: wiki-news-300d-1M.vec   \n"
     ]
    }
   ],
   "source": [
    "!unzip wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSeBiAiqhlR_"
   },
   "source": [
    "## Generate Pre-trained Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FatWEAzF34n"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(t.word_index)\n",
    "EMBED_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Mz5A60JEajbD"
   },
   "outputs": [],
   "source": [
    "word2idx = t.word_index\n",
    "FASTTEXT_INIT_EMBEDDINGS_FILE = './wiki-news-300d-1M.vec'\n",
    "\n",
    "\n",
    "def load_pretrained_embeddings(word_to_index, max_features, embedding_size, embedding_file_path):  \n",
    "    \"\"\"\n",
    "    Utility function to load the pre-trained embeddings\n",
    "    \"\"\"  \n",
    "    \n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*row.split(\" \")) \n",
    "                                for row in open(embedding_file_path, encoding=\"utf8\", errors='ignore') \n",
    "                                    if len(row)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    nb_words = min(max_features, len(word_to_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_size))\n",
    "    \n",
    "    for word, idx in word_to_index.items():\n",
    "        if idx >= max_features: \n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0WfOQV3lajbF",
    "outputId": "c7e3ba1a-424c-4bb1-ad28-73e12ce2cf3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19098, 300)"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get FastText embeddings based on our word to index mapping dictionary\n",
    "ft_embeddings = load_pretrained_embeddings(word_to_index=word2idx, \n",
    "                                           max_features=VOCAB_SIZE, \n",
    "                                           embedding_size=EMBED_SIZE, \n",
    "                                           embedding_file_path=FASTTEXT_INIT_EMBEDDINGS_FILE)\n",
    "ft_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEV8ZEN8ajbH"
   },
   "source": [
    "## Build Model Architecture\n",
    "\n",
    "We will use the ``tensorflow.keras`` high level API for building our deep neural network. One slight modification is required for the ``Embedding`` layer. In place of initializing this layer with random weights (as is usual), we start from FastText embeddings weights by setting the ``weights`` parameter. We also keep ``trainable`` parameter as ``True`` in order to learn/improve the pretrained weights as per our corpus. The rest of the model has usual ``Conv1D`` and ``MaxPool`` layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdwiN5AvBh-_"
   },
   "source": [
    "### Build a 1D-Convolution based classification model. Initialize the embedding layer with FastText weights\n",
    "\n",
    "___You can use a similar architecture as the tutorial or build your own!___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gbpb7gd3ajbH",
    "outputId": "18bb2427-ffa0-4a61-9123-29768e7036d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 250, 300)          5729400   \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 250, 256)          307456    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 125, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 62, 64)            32832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 31, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1984)              0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 256)               508160    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 6,775,097\n",
      "Trainable params: 6,775,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# the weights of the embedding layer are FastText weights\n",
    "model.add(tf.keras.layers.Embedding(VOCAB_SIZE, EMBED_SIZE,\n",
    "                                    weights=[ft_embeddings],\n",
    "                                    trainable=True,\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "model.add(tf.keras.layers.Conv1D(filters=256, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgU-nHZ2ajbJ"
   },
   "source": [
    "## Train and Validate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSfxfmWKB2yR"
   },
   "source": [
    "### Train the Model\n",
    "\n",
    "Use a similar methodology as the tutorial but use the following configs also:\n",
    "- __`validation_split`__ of __0.02__ i.e. 2%\n",
    "- 5 epochs\n",
    "- 128 batch size\n",
    "- no callbacks needed to keep things simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "8vXP4VAtF7ri"
   },
   "outputs": [],
   "source": [
    "EPOCHS=5\n",
    "BATCH_SIZE=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IpegUjivajbJ",
    "outputId": "28b5e922-e860-4ec0-d633-18a3807d73bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "249/249 [==============================] - 20s 79ms/step - loss: 0.6629 - accuracy: 0.5847 - val_loss: 0.5227 - val_accuracy: 0.7185\n",
      "Epoch 2/5\n",
      "249/249 [==============================] - 20s 79ms/step - loss: 0.3993 - accuracy: 0.8156 - val_loss: 0.4174 - val_accuracy: 0.7846\n",
      "Epoch 3/5\n",
      "249/249 [==============================] - 20s 78ms/step - loss: 0.2366 - accuracy: 0.8977 - val_loss: 0.5294 - val_accuracy: 0.7785\n",
      "Epoch 4/5\n",
      "249/249 [==============================] - 20s 79ms/step - loss: 0.1456 - accuracy: 0.9410 - val_loss: 0.6213 - val_accuracy: 0.7785\n",
      "Epoch 5/5\n",
      "249/249 [==============================] - 20s 79ms/step - loss: 0.0906 - accuracy: 0.9646 - val_loss: 0.6682 - val_accuracy: 0.7892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdfb0661d90>"
      ]
     },
     "execution_count": 77,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, \n",
    "          validation_split=0.02,\n",
    "          epochs=EPOCHS, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          shuffle=True,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-8mIjSHajbL"
   },
   "source": [
    "## Model Performance Evaluation on the Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Omex_E4B7zC"
   },
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "NXJm1NEHajbL",
    "outputId": "7d8e4b7b-9096-47fd-a7c7-998779c1576e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.82%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80      4401\n",
      "           1       0.77      0.74      0.75      3724\n",
      "\n",
      "    accuracy                           0.78      8125\n",
      "   macro avg       0.78      0.78      0.78      8125\n",
      "weighted avg       0.78      0.78      0.78      8125\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3564</td>\n",
       "      <td>837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>965</td>\n",
       "      <td>2759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  3564   837\n",
       "1   965  2759"
      ]
     },
     "execution_count": 78,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(X_test, batch_size=2048, verbose=0).ravel()\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test, predictions)*100))\n",
    "print(classification_report(y_test, predictions))\n",
    "pd.DataFrame(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKFT6X21o1Wf"
   },
   "source": [
    "___Do you observe a better performance?___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHgcPoPoajb-"
   },
   "source": [
    "# __Question 5:__ Build Model 2: Neural Network Language Model\n",
    "\n",
    "Authors Bengio et. al. in their paper titled [A Neural Probabilistic Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) present a novel method to learn the joint probability function of sequences of\n",
    "words in a language. This LM learns useful representation of sentences and words which can be leveraged for other NLP tasks such as Classication, Translation, etc.\n",
    "\n",
    "Let us leverage NNLM embeddings to train a classifier on the hate speech dataset\n",
    "\n",
    "![](https://i.imgur.com/blaLxUp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpIVtIJ6ajb_"
   },
   "source": [
    "## Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "jcXeFfnlFDNf"
   },
   "outputs": [],
   "source": [
    "norm_train_reviews = np.array(norm_train_reviews)\n",
    "norm_test_reviews = np.array(norm_test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f94Uo3GNajcC"
   },
   "source": [
    "## Build a NNLM Embedding Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cvwlBRF9ajcD",
    "outputId": "ad986651-d35e-4887-d9eb-587d823b4bd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 8 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdfb0309320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 8 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdfb0309320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 9 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdfb0309b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 9 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdfb0309b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "model = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\"\n",
    "hub_layer = hub.KerasLayer(model, output_shape=[128], input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tH3T6A_PajcF"
   },
   "source": [
    "## Build Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idbNviTuCCtX"
   },
   "source": [
    "### Build a Classification Model using the TF_Hub pretrained model\n",
    "\n",
    "___Use a similar architecture as the tutorial or try your own!___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5QlogyoajcG",
    "outputId": "6dc9dfde-1847-4dab-f654-cd1f65249a53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_4 (KerasLayer)   (None, 128)               124642688 \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 124,675,841\n",
      "Trainable params: 124,675,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(hub_layer)\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5MprbzDajcH"
   },
   "source": [
    "## Train and Validate Model\n",
    "\n",
    "Use a similar methodology as the tutorial but use the following configs also:\n",
    "- __`validation_split`__ of __0.02__ i.e. 2%\n",
    "- 5 epochs\n",
    "- 128 batch size\n",
    "- no callbacks needed to keep things simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6PnrvpPlajcJ",
    "outputId": "6492ec5a-75b2-4a11-94c4-ea0782136f46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "249/249 [==============================] - 7s 26ms/step - loss: 0.6444 - accuracy: 0.6117 - val_loss: 0.5410 - val_accuracy: 0.7031\n",
      "Epoch 2/5\n",
      "249/249 [==============================] - 6s 25ms/step - loss: 0.4596 - accuracy: 0.7801 - val_loss: 0.5096 - val_accuracy: 0.7323\n",
      "Epoch 3/5\n",
      "249/249 [==============================] - 6s 25ms/step - loss: 0.3535 - accuracy: 0.8366 - val_loss: 0.5109 - val_accuracy: 0.7508\n",
      "Epoch 4/5\n",
      "249/249 [==============================] - 6s 25ms/step - loss: 0.2665 - accuracy: 0.8825 - val_loss: 0.5514 - val_accuracy: 0.7615\n",
      "Epoch 5/5\n",
      "249/249 [==============================] - 6s 25ms/step - loss: 0.2085 - accuracy: 0.9088 - val_loss: 0.6827 - val_accuracy: 0.7677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdfb02e03d0>"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE=128\n",
    "\n",
    "# Fit the model\n",
    "model.fit(norm_train_reviews, y_train, \n",
    "          validation_split=0.02,\n",
    "          epochs=EPOCHS, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          shuffle=True,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N68uqUGGajcK"
   },
   "source": [
    "## Model Performance Evaluation on the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "x008skIhajcK",
    "outputId": "808233fd-3e5c-4bd1-eef8-6ce6f1f53477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.60%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.81      0.78      4401\n",
      "           1       0.75      0.67      0.71      3724\n",
      "\n",
      "    accuracy                           0.75      8125\n",
      "   macro avg       0.75      0.74      0.74      8125\n",
      "weighted avg       0.75      0.75      0.74      8125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3576</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1239</td>\n",
       "      <td>2485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  3576   825\n",
       "1  1239  2485"
      ]
     },
     "execution_count": 84,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(norm_test_reviews, \n",
    "                                    batch_size=512, verbose=0).ravel()\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test, predictions)*100))\n",
    "print(classification_report(y_test, predictions))\n",
    "pd.DataFrame(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBP41pQ2ajbo"
   },
   "source": [
    "# __Question 6:__ Build Model 3: Google's Universal Sentence Encoder\n",
    "\n",
    "These models take as input English strings and\n",
    "produce as output a fixed dimensional embedding\n",
    "representation of the string.\n",
    "\n",
    "It has two models for encoding\n",
    "sentences into embedding vectors. \n",
    "- One makes use\n",
    "of the transformer (Vaswani et al., 2017) architecture\n",
    "- The other is formulated as a deep averaging network (DAN) (Iyyer et al., 2015)\n",
    "\n",
    "__Methodology 1: Transformers__\n",
    "\n",
    "- The transformer based sentence encoding model\n",
    "constructs sentence embeddings using the encoding sub-graph of the transformer architecture\n",
    "(Vaswani et al., 2017). \n",
    "- This sub-graph uses attention to compute context aware representations\n",
    "of words in a sentence that take into account both\n",
    "the ordering and identity of all the other words.\n",
    "- The context aware word representations are converted to a fixed length sentence encoding vector\n",
    "by computing the element-wise sum of the representations at each word position\n",
    "- The encoder takes as input a lowercased (Penn TreeBank) PTB tokenized string\n",
    "and outputs a 512 dimensional vector as the sentence embedding\n",
    "\n",
    "\n",
    "__Methodology 2: Deep Averaging Network (DAN)__\n",
    "\n",
    "- In the deep averaging network (DAN) (Iyyer et al.,\n",
    "2015) the input embeddings for words and\n",
    "bi-grams are first averaged together and then\n",
    "passed through a feedforward deep neural network\n",
    "(DNN) to produce sentence embeddings. \n",
    "- Similar to the Transformer encoder, the DAN encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional sentence embedding.\n",
    "\n",
    "__Training Methodology:__\n",
    "\n",
    "The encoding model is designed to be as general purpose as possible. This is accomplished by using multi-task learning whereby a single encoding model is used to feed multiple downstream tasks. \n",
    "\n",
    "Unsupervised training data for the sentence encoding models are drawn from a variety of web sources. The sources are Wikipedia, web news,\n",
    "web question-answer pages and discussion forums. We augment unsupervised learning with training on supervised data from the Stanford Natural Language Inference (SNLI) corpus.\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/HIeb3tY.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7D93uotFajbx"
   },
   "source": [
    "## Build a USE Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwwGuYNWCU1Y"
   },
   "source": [
    "### Using Tensorflow hub, prepare an instance of ``hub.KerasLayer`` to get sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W1OT_A00jTHy",
    "outputId": "2a634c88-b85c-4fc1-d4d5-a53e69869ba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 10 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf30653560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 10 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf30653560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "model = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "hub_layer = hub.KerasLayer(model, output_shape=[512], input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYjvFE7dajb0"
   },
   "source": [
    "## Build Model Architecture\n",
    "\n",
    "___Use a similar architecture as the tutorial or try your own!___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3nWJfhdjp8w",
    "outputId": "ca9c5fa2-7833-4abc-a702-61b612702ad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_5 (KerasLayer)   (None, 512)               256797824 \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 256,995,201\n",
      "Trainable params: 256,995,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(hub_layer)\n",
    "\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjYqKLkMajb2"
   },
   "source": [
    "## Train and Validate Model\n",
    "\n",
    "Use a similar methodology as the tutorial but use the following configs also:\n",
    "- __`validation_split`__ of __0.02__ i.e. 2%\n",
    "- 5 epochs\n",
    "- 128 batch size\n",
    "- no callbacks needed to keep things simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DPGdUWzFj5al",
    "outputId": "0bf1b739-3d95-4da7-8a12-4d94c0497db5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "249/249 [==============================] - 24s 77ms/step - loss: 0.5868 - accuracy: 0.6756 - val_loss: 0.4660 - val_accuracy: 0.7554\n",
      "Epoch 2/5\n",
      "249/249 [==============================] - 19s 76ms/step - loss: 0.3154 - accuracy: 0.8544 - val_loss: 0.4352 - val_accuracy: 0.7600\n",
      "Epoch 3/5\n",
      "249/249 [==============================] - 19s 77ms/step - loss: 0.1654 - accuracy: 0.9302 - val_loss: 0.6532 - val_accuracy: 0.7769\n",
      "Epoch 4/5\n",
      "249/249 [==============================] - 19s 76ms/step - loss: 0.0890 - accuracy: 0.9640 - val_loss: 0.7496 - val_accuracy: 0.7738\n",
      "Epoch 5/5\n",
      "249/249 [==============================] - 19s 75ms/step - loss: 0.0584 - accuracy: 0.9762 - val_loss: 0.7934 - val_accuracy: 0.7862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdfb033fa50>"
      ]
     },
     "execution_count": 87,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "EPOCHS=5\n",
    "BATCH_SIZE=128\n",
    "    \n",
    "model.fit(norm_train_reviews, y_train, \n",
    "          validation_split=0.02,\n",
    "          epochs=EPOCHS, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          shuffle=True,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmPBH7Tfajb8"
   },
   "source": [
    "## Model Performance Evaluation on the Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSWps6DsgqoO"
   },
   "source": [
    "### **Question 9**: Get Evaluation Results of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "febzhocgknWs",
    "outputId": "aff05220-31de-4c51-a1a6-294a0e088e96"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.75%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.78      0.76      4401\n",
      "           1       0.72      0.69      0.71      3724\n",
      "\n",
      "    accuracy                           0.74      8125\n",
      "   macro avg       0.74      0.73      0.73      8125\n",
      "weighted avg       0.74      0.74      0.74      8125\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3422</td>\n",
       "      <td>979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1154</td>\n",
       "      <td>2570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  3422   979\n",
       "1  1154  2570"
      ]
     },
     "execution_count": 88,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(test_reviews, batch_size=512, verbose=0).ravel()\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test, predictions)*100))\n",
    "print(classification_report(y_test, predictions))\n",
    "pd.DataFrame(confusion_matrix(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise 1 - Text Classification with Deep Transfer Learning - Solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
